{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ZyJ5vZvWSFF"
   },
   "outputs": [],
   "source": [
    "[1]*10**10 #get more ram quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iemvQydnWlve",
    "outputId": "721ca22e-7a89-4468-fd98-2c209c0e05aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
      "/gdrive/My Drive/CDIP Dataset\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    " \n",
    "drive.mount('/gdrive')\n",
    "# the project's folder\n",
    "%cd /gdrive/'My Drive'/CDIP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "D069kUIaXCRL",
    "outputId": "73355e3b-aae0-4779-e235-51fd236f13e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0CP7GDhXHPi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score,f1_score, recall_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from numpy.random import seed\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Dense, Input, Conv1D, MaxPool1D, Concatenate, Flatten, Dropout\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Flatten,Dropout, Concatenate, Conv1D\n",
    "from keras.layers import ZeroPadding1D, Activation, GlobalMaxPool1D\n",
    "from keras.regularizers import l2\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.models import Model, load_model, Input\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "import seaborn as sns\n",
    "from keras.utils import plot_model\n",
    "from numpy.random import seed\n",
    "from mlxtend.evaluate import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXBh2C_vXzIo"
   },
   "outputs": [],
   "source": [
    "ensemble_labels = ['Adver','email','handwriten','letter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "HCermtM_X3mC",
    "outputId": "94794f54-a82d-41e6-a703-c8fce39db86c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gdrive/My Drive/CDIP Dataset/cvl/train_data/Adver\n",
      "/gdrive/My Drive/CDIP Dataset/cvl/train_data/email\n",
      "/gdrive/My Drive/CDIP Dataset/cvl/train_data/handwriten\n",
      "/gdrive/My Drive/CDIP Dataset/cvl/train_data/letter\n",
      "Length of Training Text: 1310\n",
      "Length of  Training Labels: 1310\n"
     ]
    }
   ],
   "source": [
    "#for training data\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "train_image = []\n",
    "not_read=[]\n",
    "include_extension =['txt']\n",
    "#train_dir = os.path.join(Dataset_Dir,'train')\n",
    "for label_index, label_name in enumerate(ensemble_labels):\n",
    "  base = '/gdrive/My Drive/CDIP Dataset/cvl/train_data/'+label_name\n",
    "  files = [fn for fn in os.listdir(base) if any(fn.endswith(ext) for ext in include_extension)]\n",
    "  for  file in files:\n",
    "    \n",
    "    fr = codecs.open(base+'/'+file,encoding='utf-8',errors='ignore')\n",
    "    img_ext=file.split('.')[0]\n",
    "    \n",
    "    text =fr.read().replace('\\n','')\n",
    "\n",
    "    image=cv2.imread(base + '/' + img_ext + '.tif')\n",
    "    image = cv2.resize(image, (229,229))\n",
    "    train_image.append(image)\n",
    "    train_texts.append(text)\n",
    "    train_labels.append(label_index)\n",
    "    fr.close()\n",
    "      \n",
    "           \n",
    "print('Length of Training Text: %s' % len(train_texts))\n",
    "print('Length of  Training Labels: %s' % len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "FsPc87aVX9M6",
    "outputId": "a0543c88-697f-4f13-d01f-802741a481cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Test Text: 189\n",
      "Length of  Test Labels: 189\n"
     ]
    }
   ],
   "source": [
    "#for training data\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "test_image = []\n",
    "include_extension =['txt']\n",
    "\n",
    "for label_index, label_name in enumerate(ensemble_labels):\n",
    "  base = '/gdrive/My Drive/CDIP Dataset/cvl/test_data/'+label_name\n",
    "  #text files selection\n",
    "  files = [fn for fn in os.listdir(base) if any(fn.endswith(ext) for ext in include_extension)]\n",
    "  for  file in files:\n",
    "\n",
    "    fr = codecs.open(base+'/'+file,encoding='utf-8',errors='ignore')\n",
    "    img_ext=file.split('.')[0]\n",
    "    image=cv2.imread(base + '/' + img_ext + '.tif')\n",
    "    image = cv2.resize(image, (229,229))\n",
    "    test_image.append(image)\n",
    "    test_texts.append(fr.read().replace('\\n', ''))\n",
    "    test_labels.append(label_index)\n",
    "    fr.close()\n",
    "      \n",
    "           \n",
    "print('Length of Test Text: %s' % len(test_texts))\n",
    "print('Length of  Test Labels: %s' % len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ka_iJ8WZBVY"
   },
   "outputs": [],
   "source": [
    "corpus =train_texts + test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RryVBUjTZIbE",
    "outputId": "8b421a23-a49b-4c94-e09b-0c9a44d2a5b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35511 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "maxlen = 200    #doc len\n",
    "# loading the vocab \n",
    "import pickle\n",
    "with open('/gdrive/My Drive/CDIP Dataset/cvl/Vocab/tokenizer_voc.pickle', 'rb') as tokenizer_vocab:\n",
    "  tokenizer = pickle.load(tokenizer_vocab)\n",
    "tokenizer.fit_on_texts(corpus)   \n",
    "print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
    "\n",
    "\n",
    "train_texts = np.asarray(tokenizer.texts_to_sequences(train_texts))\n",
    "test_texts = np.asarray(tokenizer.texts_to_sequences(test_texts))\n",
    "\n",
    "\n",
    "train_texts = pad_sequences(train_texts, maxlen=maxlen)\n",
    "test_texts = pad_sequences(test_texts, maxlen=maxlen)\n",
    "\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels )\n",
    "\n",
    "\n",
    "X_train, y_train = train_texts, train_labels\n",
    "X_test, y_test = test_texts, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96Qotbp4ZqGf"
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_image)):\n",
    "  image = preprocess_input(train_image[i]) \n",
    "  train_image[i] = image\n",
    "\n",
    "for i in range(len(test_image)):\n",
    "  image = preprocess_input(test_image[i]) \n",
    "  test_image[i] = image\n",
    "    \n",
    "    \n",
    "train_image = np.asarray(train_image)\n",
    "test_image = np.asarray(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kIraSLxxZ2X4"
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "tPXP9P4uUG3c",
    "outputId": "a96b3b86-8a70-4f24-ee67-b432edf892ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1\n",
      "embedding\n",
      "embedding_1\n",
      "embedding_2\n",
      "zero_padding1d\n",
      "zero_padding1d_1\n",
      "zero_padding1d_2\n",
      "zero_padding1d_3\n",
      "zero_padding1d_4\n",
      "zero_padding1d_5\n",
      "conv1d\n",
      "conv1d_1\n",
      "conv1d_2\n",
      "conv1d_3\n",
      "conv1d_4\n",
      "conv1d_5\n",
      "global_max_pooling1d\n",
      "global_max_pooling1d_1\n",
      "global_max_pooling1d_2\n",
      "global_max_pooling1d_3\n",
      "global_max_pooling1d_4\n",
      "global_max_pooling1d_5\n",
      "concatenate\n",
      "dense\n",
      "dense_1\n"
     ]
    }
   ],
   "source": [
    "model_weights ='/gdrive/My Drive/CDIP Dataset/cvl/text_model_cvl_cdip.hdf5' \n",
    "model_text = load_model(model_weights)\n",
    "layers = model_text.layers\n",
    "for layer in layers:\n",
    "  print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JRuh6MlIYxR6",
    "outputId": "7367ffc3-8f32-4310-aaf1-076a394abd0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_3\n",
      "conv2d_188\n",
      "batch_normalization_188\n",
      "activation_188\n",
      "conv2d_189\n",
      "batch_normalization_189\n",
      "activation_189\n",
      "conv2d_190\n",
      "batch_normalization_190\n",
      "activation_190\n",
      "max_pooling2d_8\n",
      "conv2d_191\n",
      "batch_normalization_191\n",
      "activation_191\n",
      "conv2d_192\n",
      "batch_normalization_192\n",
      "activation_192\n",
      "max_pooling2d_9\n",
      "conv2d_196\n",
      "batch_normalization_196\n",
      "activation_196\n",
      "conv2d_194\n",
      "conv2d_197\n",
      "batch_normalization_194\n",
      "batch_normalization_197\n",
      "activation_194\n",
      "activation_197\n",
      "average_pooling2d_18\n",
      "conv2d_193\n",
      "conv2d_195\n",
      "conv2d_198\n",
      "conv2d_199\n",
      "batch_normalization_193\n",
      "batch_normalization_195\n",
      "batch_normalization_198\n",
      "batch_normalization_199\n",
      "activation_193\n",
      "activation_195\n",
      "activation_198\n",
      "activation_199\n",
      "mixed0\n",
      "conv2d_203\n",
      "batch_normalization_203\n",
      "activation_203\n",
      "conv2d_201\n",
      "conv2d_204\n",
      "batch_normalization_201\n",
      "batch_normalization_204\n",
      "activation_201\n",
      "activation_204\n",
      "average_pooling2d_19\n",
      "conv2d_200\n",
      "conv2d_202\n",
      "conv2d_205\n",
      "conv2d_206\n",
      "batch_normalization_200\n",
      "batch_normalization_202\n",
      "batch_normalization_205\n",
      "batch_normalization_206\n",
      "activation_200\n",
      "activation_202\n",
      "activation_205\n",
      "activation_206\n",
      "mixed1\n",
      "conv2d_210\n",
      "batch_normalization_210\n",
      "activation_210\n",
      "conv2d_208\n",
      "conv2d_211\n",
      "batch_normalization_208\n",
      "batch_normalization_211\n",
      "activation_208\n",
      "activation_211\n",
      "average_pooling2d_20\n",
      "conv2d_207\n",
      "conv2d_209\n",
      "conv2d_212\n",
      "conv2d_213\n",
      "batch_normalization_207\n",
      "batch_normalization_209\n",
      "batch_normalization_212\n",
      "batch_normalization_213\n",
      "activation_207\n",
      "activation_209\n",
      "activation_212\n",
      "activation_213\n",
      "mixed2\n",
      "conv2d_215\n",
      "batch_normalization_215\n",
      "activation_215\n",
      "conv2d_216\n",
      "batch_normalization_216\n",
      "activation_216\n",
      "conv2d_214\n",
      "conv2d_217\n",
      "batch_normalization_214\n",
      "batch_normalization_217\n",
      "activation_214\n",
      "activation_217\n",
      "max_pooling2d_10\n",
      "mixed3\n",
      "conv2d_222\n",
      "batch_normalization_222\n",
      "activation_222\n",
      "conv2d_223\n",
      "batch_normalization_223\n",
      "activation_223\n",
      "conv2d_219\n",
      "conv2d_224\n",
      "batch_normalization_219\n",
      "batch_normalization_224\n",
      "activation_219\n",
      "activation_224\n",
      "conv2d_220\n",
      "conv2d_225\n",
      "batch_normalization_220\n",
      "batch_normalization_225\n",
      "activation_220\n",
      "activation_225\n",
      "average_pooling2d_21\n",
      "conv2d_218\n",
      "conv2d_221\n",
      "conv2d_226\n",
      "conv2d_227\n",
      "batch_normalization_218\n",
      "batch_normalization_221\n",
      "batch_normalization_226\n",
      "batch_normalization_227\n",
      "activation_218\n",
      "activation_221\n",
      "activation_226\n",
      "activation_227\n",
      "mixed4\n",
      "conv2d_232\n",
      "batch_normalization_232\n",
      "activation_232\n",
      "conv2d_233\n",
      "batch_normalization_233\n",
      "activation_233\n",
      "conv2d_229\n",
      "conv2d_234\n",
      "batch_normalization_229\n",
      "batch_normalization_234\n",
      "activation_229\n",
      "activation_234\n",
      "conv2d_230\n",
      "conv2d_235\n",
      "batch_normalization_230\n",
      "batch_normalization_235\n",
      "activation_230\n",
      "activation_235\n",
      "average_pooling2d_22\n",
      "conv2d_228\n",
      "conv2d_231\n",
      "conv2d_236\n",
      "conv2d_237\n",
      "batch_normalization_228\n",
      "batch_normalization_231\n",
      "batch_normalization_236\n",
      "batch_normalization_237\n",
      "activation_228\n",
      "activation_231\n",
      "activation_236\n",
      "activation_237\n",
      "mixed5\n",
      "conv2d_242\n",
      "batch_normalization_242\n",
      "activation_242\n",
      "conv2d_243\n",
      "batch_normalization_243\n",
      "activation_243\n",
      "conv2d_239\n",
      "conv2d_244\n",
      "batch_normalization_239\n",
      "batch_normalization_244\n",
      "activation_239\n",
      "activation_244\n",
      "conv2d_240\n",
      "conv2d_245\n",
      "batch_normalization_240\n",
      "batch_normalization_245\n",
      "activation_240\n",
      "activation_245\n",
      "average_pooling2d_23\n",
      "conv2d_238\n",
      "conv2d_241\n",
      "conv2d_246\n",
      "conv2d_247\n",
      "batch_normalization_238\n",
      "batch_normalization_241\n",
      "batch_normalization_246\n",
      "batch_normalization_247\n",
      "activation_238\n",
      "activation_241\n",
      "activation_246\n",
      "activation_247\n",
      "mixed6\n",
      "conv2d_252\n",
      "batch_normalization_252\n",
      "activation_252\n",
      "conv2d_253\n",
      "batch_normalization_253\n",
      "activation_253\n",
      "conv2d_249\n",
      "conv2d_254\n",
      "batch_normalization_249\n",
      "batch_normalization_254\n",
      "activation_249\n",
      "activation_254\n",
      "conv2d_250\n",
      "conv2d_255\n",
      "batch_normalization_250\n",
      "batch_normalization_255\n",
      "activation_250\n",
      "activation_255\n",
      "average_pooling2d_24\n",
      "conv2d_248\n",
      "conv2d_251\n",
      "conv2d_256\n",
      "conv2d_257\n",
      "batch_normalization_248\n",
      "batch_normalization_251\n",
      "batch_normalization_256\n",
      "batch_normalization_257\n",
      "activation_248\n",
      "activation_251\n",
      "activation_256\n",
      "activation_257\n",
      "mixed7\n",
      "conv2d_260\n",
      "batch_normalization_260\n",
      "activation_260\n",
      "conv2d_261\n",
      "batch_normalization_261\n",
      "activation_261\n",
      "conv2d_258\n",
      "conv2d_262\n",
      "batch_normalization_258\n",
      "batch_normalization_262\n",
      "activation_258\n",
      "activation_262\n",
      "conv2d_259\n",
      "conv2d_263\n",
      "batch_normalization_259\n",
      "batch_normalization_263\n",
      "activation_259\n",
      "activation_263\n",
      "max_pooling2d_11\n",
      "mixed8\n",
      "conv2d_268\n",
      "batch_normalization_268\n",
      "activation_268\n",
      "conv2d_265\n",
      "conv2d_269\n",
      "batch_normalization_265\n",
      "batch_normalization_269\n",
      "activation_265\n",
      "activation_269\n",
      "conv2d_266\n",
      "conv2d_267\n",
      "conv2d_270\n",
      "conv2d_271\n",
      "average_pooling2d_25\n",
      "conv2d_264\n",
      "batch_normalization_266\n",
      "batch_normalization_267\n",
      "batch_normalization_270\n",
      "batch_normalization_271\n",
      "conv2d_272\n",
      "batch_normalization_264\n",
      "activation_266\n",
      "activation_267\n",
      "activation_270\n",
      "activation_271\n",
      "batch_normalization_272\n",
      "activation_264\n",
      "mixed9_0\n",
      "concatenate_4\n",
      "activation_272\n",
      "mixed9\n",
      "conv2d_277\n",
      "batch_normalization_277\n",
      "activation_277\n",
      "conv2d_274\n",
      "conv2d_278\n",
      "batch_normalization_274\n",
      "batch_normalization_278\n",
      "activation_274\n",
      "activation_278\n",
      "conv2d_275\n",
      "conv2d_276\n",
      "conv2d_279\n",
      "conv2d_280\n",
      "average_pooling2d_26\n",
      "conv2d_273\n",
      "batch_normalization_275\n",
      "batch_normalization_276\n",
      "batch_normalization_279\n",
      "batch_normalization_280\n",
      "conv2d_281\n",
      "batch_normalization_273\n",
      "activation_275\n",
      "activation_276\n",
      "activation_279\n",
      "activation_280\n",
      "batch_normalization_281\n",
      "activation_273\n",
      "mixed9_1\n",
      "concatenate_5\n",
      "activation_281\n",
      "mixed10\n",
      "global_average_pooling2d_2\n",
      "dense_4\n",
      "dense_5\n"
     ]
    }
   ],
   "source": [
    "model_weights ='/gdrive/My Drive/CDIP Dataset/cvl/image_cvl_cdip.hdf5' \n",
    "model_image= load_model(model_weights)\n",
    "layers = model_image.layers\n",
    "for layer in layers:\n",
    "  print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjtpUqf8gxwY"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_weights1 ='/gdrive/My Drive/CDIP Dataset/cvl/text_model_cvl_cdip.hdf5' \n",
    "model_text = load_model(model_weights1)\n",
    "layers = model_text.layers\n",
    "\n",
    "for layer in model_text.layers:\n",
    "  if layer.name == 'concatenate':\n",
    "    layer._name = 'concatenate_'\n",
    "\n",
    "\n",
    "model_weights2 = '/gdrive/My Drive/CDIP Dataset/cvl/image_cvl_cdip.hdf5'\n",
    "model_image = load_model(model_weights2)\n",
    "layer = model_image.get_layer('dense_1')\n",
    "layer._name = 'dense_3'\n",
    "\n",
    "avg=tf.keras.layers.Average()([model_image.get_layer('dense_5').output, model_text.get_layer('dense_3').output])\n",
    "#conc = Concatenate(axis=1, name='concatenate')([image_model.get_layer('dense_5').output, text_model.get_layer('dense_3').output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-GTcu5A4vqr"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U4YWW05N3-WF"
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"/gdrive/My Drive/CDIP Dataset/cvl/model/final.h5\",\n",
    "      monitor=\"val_acc\",\n",
    "      mode=\"min\",\n",
    "     save_best_only = True,\n",
    "      verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_acc', \n",
    "     min_delta = 0, \n",
    "    patience = 8,\n",
    "     verbose = 1,\n",
    "   restore_best_weights = True)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nbus7RWBK75A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "H18wJui6cDWT",
    "outputId": "cd013bf8-328a-41a5-a814-9c923adc8f54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.3544 - acc: 0.2733\n",
      "Epoch 00001: val_acc improved from inf to 0.25397, saving model to /gdrive/My Drive/CDIP Dataset/cvl/model/final.h5\n",
      "41/41 [==============================] - 9s 209ms/step - loss: 1.3544 - acc: 0.2733 - val_loss: 1.3685 - val_acc: 0.2540\n",
      "Epoch 2/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.2946 - acc: 0.2733\n",
      "Epoch 00002: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 1.2946 - acc: 0.2733 - val_loss: 1.3590 - val_acc: 0.2540\n",
      "Epoch 3/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.2653 - acc: 0.2809\n",
      "Epoch 00003: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 123ms/step - loss: 1.2653 - acc: 0.2809 - val_loss: 1.3397 - val_acc: 0.2857\n",
      "Epoch 4/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.2387 - acc: 0.4626\n",
      "Epoch 00004: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 125ms/step - loss: 1.2387 - acc: 0.4626 - val_loss: 1.3263 - val_acc: 0.5079\n",
      "Epoch 5/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.2140 - acc: 0.5267\n",
      "Epoch 00005: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 117ms/step - loss: 1.2140 - acc: 0.5267 - val_loss: 1.3087 - val_acc: 0.4762\n",
      "Epoch 6/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.1915 - acc: 0.5267\n",
      "Epoch 00006: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 117ms/step - loss: 1.1915 - acc: 0.5267 - val_loss: 1.3013 - val_acc: 0.4339\n",
      "Epoch 7/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.1703 - acc: 0.5267\n",
      "Epoch 00007: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 123ms/step - loss: 1.1703 - acc: 0.5267 - val_loss: 1.2596 - val_acc: 0.5132\n",
      "Epoch 8/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.1495 - acc: 0.5267\n",
      "Epoch 00008: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 117ms/step - loss: 1.1495 - acc: 0.5267 - val_loss: 1.2740 - val_acc: 0.4709\n",
      "Epoch 9/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.1305 - acc: 0.6840\n",
      "Epoch 00009: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 1.1305 - acc: 0.6840 - val_loss: 1.2569 - val_acc: 0.5132\n",
      "Epoch 10/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.1124 - acc: 0.7573\n",
      "Epoch 00010: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 123ms/step - loss: 1.1124 - acc: 0.7573 - val_loss: 1.2148 - val_acc: 0.6138\n",
      "Epoch 11/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.0951 - acc: 0.7573\n",
      "Epoch 00011: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 116ms/step - loss: 1.0951 - acc: 0.7573 - val_loss: 1.2198 - val_acc: 0.5767\n",
      "Epoch 12/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.0776 - acc: 0.7573\n",
      "Epoch 00012: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 1.0776 - acc: 0.7573 - val_loss: 1.2370 - val_acc: 0.5344\n",
      "Epoch 13/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.0608 - acc: 0.7573\n",
      "Epoch 00013: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 1.0608 - acc: 0.7573 - val_loss: 1.2111 - val_acc: 0.5344\n",
      "Epoch 14/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.0443 - acc: 0.7573\n",
      "Epoch 00014: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 117ms/step - loss: 1.0443 - acc: 0.7573 - val_loss: 1.1641 - val_acc: 0.5926\n",
      "Epoch 15/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.0282 - acc: 0.7573\n",
      "Epoch 00015: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 116ms/step - loss: 1.0282 - acc: 0.7573 - val_loss: 1.2093 - val_acc: 0.5291\n",
      "Epoch 16/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.0136 - acc: 0.7573\n",
      "Epoch 00016: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 123ms/step - loss: 1.0136 - acc: 0.7573 - val_loss: 1.1311 - val_acc: 0.6190\n",
      "Epoch 17/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.9972 - acc: 0.7573\n",
      "Epoch 00017: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 0.9972 - acc: 0.7573 - val_loss: 1.1287 - val_acc: 0.6085\n",
      "Epoch 18/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.9823 - acc: 0.7573\n",
      "Epoch 00018: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 116ms/step - loss: 0.9823 - acc: 0.7573 - val_loss: 1.1153 - val_acc: 0.6085\n",
      "Epoch 19/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.9676 - acc: 0.7573\n",
      "Epoch 00019: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 0.9676 - acc: 0.7573 - val_loss: 1.1277 - val_acc: 0.5767\n",
      "Epoch 20/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.9538 - acc: 0.7573\n",
      "Epoch 00020: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 120ms/step - loss: 0.9538 - acc: 0.7573 - val_loss: 1.1230 - val_acc: 0.5820\n",
      "Epoch 21/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.9395 - acc: 0.7573\n",
      "Epoch 00021: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 0.9395 - acc: 0.7573 - val_loss: 1.1679 - val_acc: 0.5344\n",
      "Epoch 22/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.9258 - acc: 0.7573\n",
      "Epoch 00022: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 117ms/step - loss: 0.9258 - acc: 0.7573 - val_loss: 1.1221 - val_acc: 0.5556\n",
      "Epoch 23/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.9149 - acc: 0.7565\n",
      "Epoch 00023: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 123ms/step - loss: 0.9149 - acc: 0.7565 - val_loss: 1.0442 - val_acc: 0.6296\n",
      "Epoch 24/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.8989 - acc: 0.7573\n",
      "Epoch 00024: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 117ms/step - loss: 0.8989 - acc: 0.7573 - val_loss: 1.0594 - val_acc: 0.5926\n",
      "Epoch 25/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.8856 - acc: 0.7573\n",
      "Epoch 00025: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 0.8856 - acc: 0.7573 - val_loss: 1.0569 - val_acc: 0.6085\n",
      "Epoch 26/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.8726 - acc: 0.7573\n",
      "Epoch 00026: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 116ms/step - loss: 0.8726 - acc: 0.7573 - val_loss: 1.0956 - val_acc: 0.5661\n",
      "Epoch 27/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.8611 - acc: 0.7573\n",
      "Epoch 00027: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 0.8611 - acc: 0.7573 - val_loss: 1.1126 - val_acc: 0.5450\n",
      "Epoch 28/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.8474 - acc: 0.7573\n",
      "Epoch 00028: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 0.8474 - acc: 0.7573 - val_loss: 1.0512 - val_acc: 0.5926\n",
      "Epoch 29/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.7623 - acc: 0.7374\n",
      "Epoch 00029: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 0.7623 - acc: 0.7374 - val_loss: 1.1157 - val_acc: 0.5397\n",
      "Epoch 30/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.7341 - acc: 0.7237\n",
      "Epoch 00030: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 124ms/step - loss: 0.7341 - acc: 0.7237 - val_loss: 0.8392 - val_acc: 0.6667\n",
      "Epoch 31/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.7102 - acc: 0.7313\n",
      "Epoch 00031: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 122ms/step - loss: 0.7102 - acc: 0.7313 - val_loss: 0.8161 - val_acc: 0.7090\n",
      "Epoch 32/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.6845 - acc: 0.7427\n",
      "Epoch 00032: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 117ms/step - loss: 0.6845 - acc: 0.7427 - val_loss: 0.8677 - val_acc: 0.6455\n",
      "Epoch 33/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.6818 - acc: 0.7397\n",
      "Epoch 00033: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 0.6818 - acc: 0.7397 - val_loss: 0.8237 - val_acc: 0.6931\n",
      "Epoch 34/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.5958 - acc: 0.8252\n",
      "Epoch 00034: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 124ms/step - loss: 0.5958 - acc: 0.8252 - val_loss: 0.7386 - val_acc: 0.8201\n",
      "Epoch 35/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.5558 - acc: 0.8924\n",
      "Epoch 00035: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 0.5558 - acc: 0.8924 - val_loss: 0.7359 - val_acc: 0.8201\n",
      "Epoch 36/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.5071 - acc: 0.9336\n",
      "Epoch 00036: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 124ms/step - loss: 0.5071 - acc: 0.9336 - val_loss: 0.6298 - val_acc: 0.8995\n",
      "Epoch 37/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.4930 - acc: 0.9374\n",
      "Epoch 00037: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 119ms/step - loss: 0.4930 - acc: 0.9374 - val_loss: 0.6381 - val_acc: 0.8624\n",
      "Epoch 38/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.4597 - acc: 0.9519\n",
      "Epoch 00038: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 117ms/step - loss: 0.4597 - acc: 0.9519 - val_loss: 0.7077 - val_acc: 0.8360\n",
      "Epoch 39/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.4474 - acc: 0.9550\n",
      "Epoch 00039: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 0.4474 - acc: 0.9550 - val_loss: 0.7455 - val_acc: 0.8836\n",
      "Epoch 40/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.4534 - acc: 0.9519\n",
      "Epoch 00040: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 0.4534 - acc: 0.9519 - val_loss: 0.5922 - val_acc: 0.8836\n",
      "Epoch 41/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.4095 - acc: 0.9527\n",
      "Epoch 00041: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 123ms/step - loss: 0.4095 - acc: 0.9527 - val_loss: 0.5304 - val_acc: 0.9259\n",
      "Epoch 42/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.4044 - acc: 0.9580\n",
      "Epoch 00042: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 119ms/step - loss: 0.4044 - acc: 0.9580 - val_loss: 0.5825 - val_acc: 0.8730\n",
      "Epoch 43/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.3790 - acc: 0.9672\n",
      "Epoch 00043: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 119ms/step - loss: 0.3790 - acc: 0.9672 - val_loss: 0.6150 - val_acc: 0.8836\n",
      "Epoch 44/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.3592 - acc: 0.9702\n",
      "Epoch 00044: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 119ms/step - loss: 0.3592 - acc: 0.9702 - val_loss: 0.5621 - val_acc: 0.8624\n",
      "Epoch 45/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.3553 - acc: 0.9634\n",
      "Epoch 00045: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 119ms/step - loss: 0.3553 - acc: 0.9634 - val_loss: 0.7895 - val_acc: 0.6984\n",
      "Epoch 46/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.3555 - acc: 0.9573\n",
      "Epoch 00046: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 117ms/step - loss: 0.3555 - acc: 0.9573 - val_loss: 0.5147 - val_acc: 0.8677\n",
      "Epoch 47/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.3321 - acc: 0.9718\n",
      "Epoch 00047: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 118ms/step - loss: 0.3321 - acc: 0.9718 - val_loss: 0.5257 - val_acc: 0.8571\n",
      "Epoch 48/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.3105 - acc: 0.9756\n",
      "Epoch 00048: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 117ms/step - loss: 0.3105 - acc: 0.9756 - val_loss: 0.4735 - val_acc: 0.8942\n",
      "Epoch 49/100\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.3065 - acc: 0.9695Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.25397\n",
      "41/41 [==============================] - 5s 123ms/step - loss: 0.3065 - acc: 0.9695 - val_loss: 0.4564 - val_acc: 0.8995\n",
      "Epoch 00049: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4fcd15ddd8>"
      ]
     },
     "execution_count": 133,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds =Dense(4 ,activation='softmax', name='dense_')(avg)\n",
    "\n",
    "model_comb = Model(inputs=[model_image.input, model_text.input], outputs=preds)\n",
    "\n",
    "\n",
    "model_comb.compile(optimizer = RMSprop(lr = 0.001), loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_comb.fit([train_image, X_train], y_train,\n",
    "                   epochs=100,\n",
    "                   batch_size=32,\n",
    "                   callbacks=callbacks,\n",
    "                   validation_data=([test_image, X_test], y_test),\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "A_PobfCd72M0",
    "outputId": "11d867f3-7dcb-4b32-f924-481c66c8e7a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 31ms/step - loss: 0.5304 - acc: 0.9259\n",
      "[0.5303651690483093, 0.9259259104728699]\n",
      "6/6 [==============================] - 0s 29ms/step\n",
      "[[44  0  1  0]\n",
      " [ 0 48  0  0]\n",
      " [ 8  0 46  0]\n",
      " [ 0  2  3 37]]\n"
     ]
    }
   ],
   "source": [
    "res = model_comb.evaluate([test_image, X_test], y_test)\n",
    "print(res)\n",
    "y_preds=model_comb.predict([test_image, X_test],batch_size=32, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "MXPULuB-DYHC",
    "outputId": "bb2f8cb8-e9c9-42ad-f24d-44ed1d6c9d00"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_target=y_test.argmax(axis=1),\n",
    "                      y_predicted=y_preds.argmax(axis=1),\n",
    "                      binary=False)\n",
    "\n",
    "figure, axes = plot_confusion_matrix(conf_mat=conf_matrix)\n",
    "plt.xlabel('Labels Predicted ')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "combined_text_image_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
