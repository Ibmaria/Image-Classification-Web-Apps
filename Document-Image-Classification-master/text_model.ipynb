{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_model.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Inek_uNVVZ9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[1]*10**10 #get more ram quickly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1NBujP0V-P0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c3a1c013-2c7a-474e-e549-e0aed2a1bd9b"
      },
      "source": [
        "from google.colab import drive\n",
        " \n",
        "drive.mount('/gdrive')\n",
        "# the project's folder\n",
        "%cd /gdrive/'My Drive'/CDIP Dataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/My Drive/CDIP Dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et3ug_yLWf06",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e6d20ca-f36a-4e6b-c7ba-ac85d14f5d47"
      },
      "source": [
        "#I unzip images folder\n",
        "from zipfile import ZipFile\n",
        "filename='test_data.zip'\n",
        "with ZipFile(filename,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print(\"done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tTXDehpTWie",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b820da2b-3b37-44a7-c46f-6a8991f639d8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00kGR2t6Y-qF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e44b7bfb-598a-4351-b967-005cd0aa8a7a"
      },
      "source": [
        "import os\n",
        "import codecs\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score,f1_score, recall_score, confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Embedding, Dense, Input, Conv1D, MaxPool1D, Concatenate, Flatten, Dropout\n",
        "from keras.layers import ZeroPadding1D, Activation, GlobalMaxPool1D\n",
        "from keras.regularizers import l2\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRDZP21ZaAzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import asarray\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olbFB1YKZU71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_fast_test_embeddings(max_words, best_feat_list, tokenizer_index):\n",
        "    f = open('/gdrive/My Drive/CDIP Dataset/cvl/Fast_text-emb/wiki-news-300d-1M.vec')\n",
        "    embeddings_index = {}\n",
        "    comp=0\n",
        "    for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs =asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "    print('Found %s word vectors.' % len(embeddings_index))\n",
        "    embedding_dim = 300 # fastext dim \n",
        "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "    for word, i in tokenizer_index.items():\n",
        "      if i < max_words and word in best_feat_list:\n",
        "        comp +=1\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "    print(\"comp\",comp)\n",
        "    return embedding_dim, embedding_matrix\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kP4QOc_epsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ensemble_labels = ['Adver','email','handwriten','letter']"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3103wpQZ-75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "476d78a2-3405-439c-e7bf-b6761b79c88a"
      },
      "source": [
        "#for training data\n",
        "texts_train = []\n",
        "labels_train = []\n",
        "not_read=[]\n",
        "include_extension =['txt']\n",
        "#train_dir = os.path.join(Dataset_Dir,'train')\n",
        "for label_index, label_name in enumerate(ensemble_labels):\n",
        "  base = '/gdrive/My Drive/CDIP Dataset/cvl/train_data/'+label_name\n",
        "  print(base)\n",
        "  #text files selection\n",
        "  files = [fn for fn in os.listdir(base) if any(fn.endswith(ext) for ext in include_extension)]\n",
        "  for  file in files:\n",
        "    #print(file)\n",
        "    fr = codecs.open(base+'/'+file,encoding='utf-8',errors='ignore')\n",
        "    #print(\"ok1\")\n",
        "\n",
        "    #text =fr.read()\n",
        "    #if not fr.read():\n",
        "    #  not_read.append(file)\n",
        "    #  continue\n",
        "    text =fr.read().replace('\\n','')\n",
        "\n",
        "    #print(\"ok2\")\n",
        "    #print(fr.read())\n",
        "    texts_train.append(text)\n",
        "    labels_train.append(label_index)\n",
        "    fr.close()\n",
        "      \n",
        "           \n",
        "print('Length of Training Text: %s' % len(texts_train))\n",
        "print('Length of  Training Labels: %s' % len(labels_train))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/CDIP Dataset/cvl/train_data/Adver\n",
            "/gdrive/My Drive/CDIP Dataset/cvl/train_data/email\n",
            "/gdrive/My Drive/CDIP Dataset/cvl/train_data/handwriten\n",
            "/gdrive/My Drive/CDIP Dataset/cvl/train_data/letter\n",
            "Length of Training Text: 1310\n",
            "Length of  Training Labels: 1310\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OKshHqdR5Du",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5a0a77c7-dcd5-493d-ba7b-61f0c097a4d1"
      },
      "source": [
        "texts_train[10]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'last hunt, major hooumsmoked cigarotte stamped withhis family crest,now everybody stamped ownfamily crest, .walmost then, try tobe.)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bonxwAXBiRbr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "04bd9538-cd81-4a1a-ad64-59841c1c9f65"
      },
      "source": [
        "#for training data\n",
        "texts_test = []\n",
        "labels_test = []\n",
        "include_extension =['txt']\n",
        "\n",
        "for label_index, label_name in enumerate(ensemble_labels):\n",
        "  base = '/gdrive/My Drive/CDIP Dataset/cvl/test_data/'+label_name\n",
        "  #text files selection\n",
        "  files = [fn for fn in os.listdir(base) if any(fn.endswith(ext) for ext in include_extension)]\n",
        "  for  file in files:\n",
        "    fr = codecs.open(base+'/'+file,encoding='utf-8',errors='ignore')\n",
        "    texts_test.append(fr.read().replace('\\n', ''))\n",
        "    labels_test.append(label_index)\n",
        "    fr.close()\n",
        "      \n",
        "           \n",
        "print('Length of Test Text: %s' % len(texts_test))\n",
        "print('Length of  Test Labels: %s' % len(labels_test))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Test Text: 189\n",
            "Length of  Test Labels: 189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsKBOdLnywJS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7aacb5df-9cd8-4bb7-b32e-646bb89cd0fc"
      },
      "source": [
        "texts_test[10]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"the new lookingold# and general's warnin' jokin:causes lung cancer, hear ase,emphy and may complicate empane)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRUzCwxmu1Ak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = texts_train + texts_test"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCKIlJMHvnrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select n features from each class\n",
        "number_of_features_each_class=500\n",
        "best_feat_list=list()\n",
        "path_results_features_selection = '/gdrive/My Drive/CDIP Dataset/cvl/best_features/'\n",
        "ensemble_labels = ['Adver','email','handwriten','letter']\n",
        "for label in ensemble_labels:\n",
        "  current_label=path_results_features_selection+label\n",
        "  filename=label+'.txt'\n",
        "  fr = codecs.open(current_label+'/'+filename,encoding='utf-8',errors='ignore')\n",
        "  best_features=fr.read().split()\n",
        "  for feature in range(0, number_of_features_each_class):\n",
        "    if feature > len(best_features) - 1:\n",
        "      break\n",
        "    if best_features[feature] not in best_feat_list:\n",
        "      best_feat_list.append(best_features[feature])\n",
        "  #select feature for label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   \n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlsSc4JgNhY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a2c0fe42-b14e-46d8-8bfe-4e31cd7944fe"
      },
      "source": [
        "best_feat_list"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['would',\n",
              " 'please',\n",
              " 'surgeon',\n",
              " \"general's\",\n",
              " 'warning:',\n",
              " 'know',\n",
              " 'and',\n",
              " 'york,',\n",
              " 'dr.',\n",
              " 'low',\n",
              " 'meeting',\n",
              " 'your',\n",
              " 'has',\n",
              " 'also',\n",
              " 'modified',\n",
              " 'information',\n",
              " 'original',\n",
              " 'smoking',\n",
              " 'new',\n",
              " 'work',\n",
              " 'tobacco',\n",
              " 'cigarette',\n",
              " 'product',\n",
              " 'research',\n",
              " 'copy',\n",
              " 'lung',\n",
              " 'york',\n",
              " 'let',\n",
              " 'forward',\n",
              " 'need',\n",
              " 'see',\n",
              " 'taste',\n",
              " 'october',\n",
              " 'time',\n",
              " 'following',\n",
              " 'january',\n",
              " 'two',\n",
              " 'women',\n",
              " 'june',\n",
              " 'next',\n",
              " 'dangerous',\n",
              " 'park',\n",
              " 'you',\n",
              " 'august',\n",
              " 'pregnant',\n",
              " 'september',\n",
              " 'hope',\n",
              " 'this',\n",
              " 'provide',\n",
              " 'call',\n",
              " 'july',\n",
              " 'february',\n",
              " 'received',\n",
              " 'tar',\n",
              " 'dear',\n",
              " 'use',\n",
              " 'find',\n",
              " 'march',\n",
              " 'like',\n",
              " 'heart',\n",
              " 'april',\n",
              " 'make',\n",
              " 'general',\n",
              " 'could',\n",
              " 'flavor',\n",
              " 'contains',\n",
              " 'list',\n",
              " 'number',\n",
              " 'office',\n",
              " 'end',\n",
              " 'november',\n",
              " 'smoke',\n",
              " 'well',\n",
              " 'give',\n",
              " 'birth',\n",
              " 'complicate',\n",
              " 'premature',\n",
              " 'might',\n",
              " 'eee',\n",
              " 'box',\n",
              " 'additional',\n",
              " 'interest',\n",
              " 'serious',\n",
              " 'week',\n",
              " 'last',\n",
              " 'current',\n",
              " 'recent',\n",
              " 'within',\n",
              " 'carbon',\n",
              " 'december',\n",
              " 'reduces',\n",
              " 'program',\n",
              " 'year',\n",
              " 'east',\n",
              " 'birth,',\n",
              " 'cancer,',\n",
              " 'the',\n",
              " 'no.',\n",
              " 'working',\n",
              " 'north',\n",
              " 'support',\n",
              " 'company',\n",
              " 'several',\n",
              " 'look',\n",
              " 'result',\n",
              " 'camels',\n",
              " 'quitting',\n",
              " '22,',\n",
              " 'able',\n",
              " 'date',\n",
              " 'asked',\n",
              " 'fetalinjury,',\n",
              " 'smokingby',\n",
              " '23,',\n",
              " '24,',\n",
              " 'ofthe',\n",
              " 'not',\n",
              " 'questions',\n",
              " 'used',\n",
              " 'philip',\n",
              " 'first',\n",
              " 'avenue,',\n",
              " 'check',\n",
              " 'soon',\n",
              " 'smokers',\n",
              " 'fic',\n",
              " 'only',\n",
              " 'risks',\n",
              " 'weight.',\n",
              " 'carolina',\n",
              " 'business',\n",
              " 'present',\n",
              " 'market',\n",
              " 'state',\n",
              " 'ina',\n",
              " 'for',\n",
              " 'page',\n",
              " 'toyour',\n",
              " 'held',\n",
              " 'provided',\n",
              " 'results',\n",
              " 'scheduled',\n",
              " 'advertising',\n",
              " 'day',\n",
              " 'every',\n",
              " 'mg.',\n",
              " 'continue',\n",
              " 'final',\n",
              " 'media',\n",
              " 'states',\n",
              " 'menthol',\n",
              " 'may',\n",
              " 'conference',\n",
              " 'response',\n",
              " 'scientific',\n",
              " 'free',\n",
              " 'greatly',\n",
              " 'world',\n",
              " 'help',\n",
              " 'pregnancy.',\n",
              " 'smokingnow',\n",
              " 'data',\n",
              " 'pleased',\n",
              " 'team',\n",
              " 'whether',\n",
              " 'try',\n",
              " 'public',\n",
              " 'right',\n",
              " 'already',\n",
              " 'young',\n",
              " 'health.',\n",
              " 'concerning',\n",
              " 'done',\n",
              " 'second',\n",
              " 'tom',\n",
              " 'change',\n",
              " 'issue',\n",
              " 'opportunity',\n",
              " 'but',\n",
              " 'site',\n",
              " 'group',\n",
              " 'slims',\n",
              " 'tat',\n",
              " 'since',\n",
              " 'cigarette,',\n",
              " 'parents',\n",
              " 'ultra',\n",
              " 'smokingcauses',\n",
              " 'sugeon',\n",
              " '16,',\n",
              " '27,',\n",
              " 'fax',\n",
              " 'including',\n",
              " 'study',\n",
              " 'early',\n",
              " 'process',\n",
              " 'set',\n",
              " 'camel',\n",
              " 'made',\n",
              " 'important',\n",
              " 'buy',\n",
              " 'children',\n",
              " 'enjoy',\n",
              " 'factor',\n",
              " 'king',\n",
              " 'monoxide.',\n",
              " 'peer',\n",
              " 'area',\n",
              " 'fact',\n",
              " 'key',\n",
              " 'past',\n",
              " 'test',\n",
              " 'reynolds',\n",
              " 'filter',\n",
              " 'american',\n",
              " 'three',\n",
              " 'kids',\n",
              " 'today.',\n",
              " 'ftc',\n",
              " '18,',\n",
              " 'center',\n",
              " 'certain',\n",
              " 'division',\n",
              " 'doral',\n",
              " 'place',\n",
              " 'cigarettes',\n",
              " 'take',\n",
              " 'say',\n",
              " 'name',\n",
              " 'today',\n",
              " 'west',\n",
              " 'any',\n",
              " 'means',\n",
              " 'cut',\n",
              " 'avenue',\n",
              " 'consider',\n",
              " 'contain',\n",
              " 'control',\n",
              " 'law',\n",
              " 'specific',\n",
              " 'street,',\n",
              " 'thought',\n",
              " 'to:',\n",
              " 'wish',\n",
              " 'report',\n",
              " 'many',\n",
              " 'america’s',\n",
              " 'cigarette!',\n",
              " 'don’t',\n",
              " 'fte',\n",
              " 'heath,',\n",
              " 'ing:',\n",
              " 'waring:',\n",
              " '‘surgeon',\n",
              " 'feel',\n",
              " \"i'm\",\n",
              " 'paper',\n",
              " 'sample',\n",
              " 'together',\n",
              " 'issues',\n",
              " 'all',\n",
              " 'order',\n",
              " 'sure',\n",
              " 'because',\n",
              " 'advertisement',\n",
              " 'smoking,',\n",
              " 'changes',\n",
              " 'frank',\n",
              " 'looking',\n",
              " 'management',\n",
              " 'programs',\n",
              " 'required',\n",
              " 'samples',\n",
              " 'various',\n",
              " 'very',\n",
              " 'famous',\n",
              " 'kings',\n",
              " 'men',\n",
              " 'method.',\n",
              " 'that’s',\n",
              " 'then',\n",
              " '28,',\n",
              " 'join',\n",
              " 'seen',\n",
              " 'our',\n",
              " 'health',\n",
              " 'bring',\n",
              " 'ate',\n",
              " 'available',\n",
              " 'que',\n",
              " 'analysis',\n",
              " 'level',\n",
              " 'line',\n",
              " 'potential',\n",
              " 'service',\n",
              " 'choice',\n",
              " 'ese',\n",
              " 'gives',\n",
              " 'helping',\n",
              " 'hon',\n",
              " \"it's\",\n",
              " 'leading',\n",
              " 'sense',\n",
              " 'tol',\n",
              " 'ask',\n",
              " 'cover',\n",
              " 'statement',\n",
              " 'meet',\n",
              " 'third',\n",
              " 'even',\n",
              " 'got',\n",
              " 'wil',\n",
              " 'never',\n",
              " 'morris',\n",
              " 'want',\n",
              " 'agency',\n",
              " 'approval',\n",
              " 'decision',\n",
              " 'develop',\n",
              " 'idea',\n",
              " 'latest',\n",
              " 'prior',\n",
              " 'probably',\n",
              " 'street',\n",
              " 'suggested',\n",
              " 'yet',\n",
              " 'winston',\n",
              " 'alot',\n",
              " 'drug',\n",
              " 'son',\n",
              " \"that's\",\n",
              " 'aware',\n",
              " 'making',\n",
              " 'medical',\n",
              " 'merit',\n",
              " 'production',\n",
              " 'question',\n",
              " 'room',\n",
              " \"100's\",\n",
              " 'appendix',\n",
              " 'cigaete',\n",
              " 'ea,',\n",
              " 'experts',\n",
              " 'filter.',\n",
              " 'introducing',\n",
              " 'laser',\n",
              " 'mater',\n",
              " 'nose',\n",
              " 'people.',\n",
              " 'straight',\n",
              " 'tenet',\n",
              " 'totally',\n",
              " 'window',\n",
              " 'air',\n",
              " 'along',\n",
              " 'complete',\n",
              " 'full',\n",
              " 'legal',\n",
              " 'sales',\n",
              " 'form',\n",
              " 'found',\n",
              " 'web',\n",
              " 'brands',\n",
              " 'regular',\n",
              " 'tae',\n",
              " 'also,',\n",
              " 'costs',\n",
              " 'developed',\n",
              " 'effects',\n",
              " 'jan',\n",
              " 'main',\n",
              " 'n.y.',\n",
              " 'seems',\n",
              " 'stated',\n",
              " 'time,',\n",
              " 'ape',\n",
              " 'certainly',\n",
              " 'family',\n",
              " 'friends',\n",
              " 'true',\n",
              " 'me.',\n",
              " 'suite',\n",
              " 'much',\n",
              " 'per',\n",
              " 'advice',\n",
              " 'ave.',\n",
              " 'be.',\n",
              " 'bith',\n",
              " 'bow',\n",
              " 'child',\n",
              " 'choices',\n",
              " 'cool',\n",
              " 'ee:',\n",
              " 'goodness',\n",
              " 'happened',\n",
              " 'heath',\n",
              " 'lowest',\n",
              " 'natural',\n",
              " 'ofall',\n",
              " 'opt',\n",
              " 'po.',\n",
              " 'smoked',\n",
              " 'sree',\n",
              " 'stop',\n",
              " 'than',\n",
              " \"there's\",\n",
              " \"they're\",\n",
              " 'throat',\n",
              " 'timn',\n",
              " 'virgina',\n",
              " 'yes,',\n",
              " 'cost',\n",
              " 'forthe',\n",
              " 'without',\n",
              " 'case',\n",
              " 'believe',\n",
              " 'ever',\n",
              " 'amount',\n",
              " 'basis',\n",
              " 'begin',\n",
              " 'cannot',\n",
              " 'che',\n",
              " 'clear',\n",
              " 'code',\n",
              " 'doctor',\n",
              " 'efforts',\n",
              " 'environmental',\n",
              " 'existing',\n",
              " 'given',\n",
              " 'inc.',\n",
              " 'indicated',\n",
              " 'ken',\n",
              " 'print',\n",
              " 'related',\n",
              " 'section',\n",
              " 'separate',\n",
              " 'something',\n",
              " 'standard',\n",
              " 'testing',\n",
              " 'tha',\n",
              " 'weeks',\n",
              " 'what',\n",
              " 'white',\n",
              " 'industry',\n",
              " 'people',\n",
              " 'with',\n",
              " 'example,',\n",
              " 'ine',\n",
              " 'red',\n",
              " 'should',\n",
              " 'from:',\n",
              " 're:',\n",
              " 'date:',\n",
              " 'touched',\n",
              " 'pmto:',\n",
              " 'date:sent',\n",
              " 'monday,',\n",
              " 'fw:',\n",
              " 'karen',\n",
              " 'amlast',\n",
              " 'amto:',\n",
              " 'wednesday,',\n",
              " 'pmlast',\n",
              " 'friday,',\n",
              " '2001-nov-20',\n",
              " 'daragan,',\n",
              " 'michael',\n",
              " 'get',\n",
              " 'attached',\n",
              " 'thursday,',\n",
              " 'message—from:',\n",
              " 'john',\n",
              " 'david',\n",
              " 'l.;',\n",
              " 'mike',\n",
              " 'tuesday,',\n",
              " 'w.;',\n",
              " 'henry',\n",
              " 'smith,',\n",
              " 'mary',\n",
              " 'rick',\n",
              " 'message',\n",
              " 'sent',\n",
              " 'john;',\n",
              " 'messagefrom:',\n",
              " 'carolyn',\n",
              " 'p.;',\n",
              " 's.;',\n",
              " 'discuss',\n",
              " 'ysp',\n",
              " '08,',\n",
              " 'j.;',\n",
              " 'm.;',\n",
              " 'mark',\n",
              " 'prevention',\n",
              " 'jim',\n",
              " 'a.;',\n",
              " 'greg',\n",
              " 'judy',\n",
              " 'mccormick,',\n",
              " 'orginal',\n",
              " 'going',\n",
              " 'roger',\n",
              " 'letter',\n",
              " 'back',\n",
              " 'datsent',\n",
              " 'file:',\n",
              " 'kevin',\n",
              " 'linda',\n",
              " 'kelly',\n",
              " 'levy,',\n",
              " 'draft',\n",
              " 'attachments:',\n",
              " 'brendan',\n",
              " 'd.;',\n",
              " 'merlo,',\n",
              " 'mike;',\n",
              " 'tomorrow',\n",
              " 'review',\n",
              " 'confidential',\n",
              " 'robert',\n",
              " 'denise',\n",
              " 'dave',\n",
              " 'friday',\n",
              " 'nancy',\n",
              " 'schedule',\n",
              " '07,',\n",
              " 'carol',\n",
              " 'chaikin,',\n",
              " 'datedate:',\n",
              " 'f.;',\n",
              " 'karen;',\n",
              " 'nelson,',\n",
              " 'sent:',\n",
              " 'threatened',\n",
              " '‘thursday,',\n",
              " 'anything',\n",
              " 'richard',\n",
              " '03,',\n",
              " 'ellen',\n",
              " 'ongoing',\n",
              " 'send',\n",
              " 'connection',\n",
              " 'bill',\n",
              " 'contact',\n",
              " 'pat',\n",
              " 'thanks',\n",
              " 'council',\n",
              " 'b.;',\n",
              " 'david;',\n",
              " 'defense,',\n",
              " 'update',\n",
              " 'note',\n",
              " 'joint',\n",
              " 'richmond',\n",
              " 'enclosed',\n",
              " 'product,',\n",
              " 'virginia',\n",
              " 'years',\n",
              " 'thomas',\n",
              " 'e-mail',\n",
              " 'just',\n",
              " 'barbara',\n",
              " 'bruce',\n",
              " 'else',\n",
              " 'susan',\n",
              " '11,',\n",
              " 'e.;',\n",
              " 'g.;',\n",
              " 'solana,',\n",
              " 'spoke',\n",
              " 'wed,',\n",
              " 'application',\n",
              " 'said',\n",
              " 'moore,',\n",
              " 'one',\n",
              " 'morning',\n",
              " 'pmusa',\n",
              " 'mail',\n",
              " 'news',\n",
              " 'george',\n",
              " '‘the',\n",
              " 'ike',\n",
              " 'someone',\n",
              " 'steve',\n",
              " 'a.subject:',\n",
              " 'afternoon',\n",
              " 'amsubject:',\n",
              " 'amtor',\n",
              " 'c.;',\n",
              " 'davis,',\n",
              " 'ellen;',\n",
              " 'message:from:',\n",
              " 'pmsubject:',\n",
              " 'privileged',\n",
              " 'robin',\n",
              " 'sanders,',\n",
              " 'timothy',\n",
              " 'institute',\n",
              " 'affairs',\n",
              " 'subject:',\n",
              " 'tee',\n",
              " 'youth',\n",
              " 'plan',\n",
              " 'nicotine',\n",
              " 'wanted',\n",
              " 'monday',\n",
              " 'attend',\n",
              " 'donald',\n",
              " '14,',\n",
              " 'telephone',\n",
              " 'them.',\n",
              " 'dates',\n",
              " 'move',\n",
              " 'updated',\n",
              " 'vice',\n",
              " 'via',\n",
              " 'cancer',\n",
              " 'university',\n",
              " '01,',\n",
              " '05,',\n",
              " 'albert;',\n",
              " 'attachments',\n",
              " 'brown,',\n",
              " 'byron',\n",
              " 'cheryl',\n",
              " 'email',\n",
              " 'frank;',\n",
              " 'importance:',\n",
              " 'message-from:',\n",
              " 'meyne,',\n",
              " 'n.;',\n",
              " 'nicoli,',\n",
              " 'pfeil,',\n",
              " 'pmte',\n",
              " 'r.;',\n",
              " 'ryan,',\n",
              " 'scruggs,',\n",
              " 't.;',\n",
              " 'walk,',\n",
              " 'whidden,',\n",
              " 'williams,',\n",
              " 'apr',\n",
              " 'inthe',\n",
              " 'high',\n",
              " 'opinion',\n",
              " 'presentation',\n",
              " 'craig',\n",
              " 'eric',\n",
              " 'jones,',\n",
              " 'scott',\n",
              " 'department',\n",
              " 'net',\n",
              " 'these',\n",
              " 'put',\n",
              " 'bob',\n",
              " 'employee',\n",
              " 'looks',\n",
              " 'objectives',\n",
              " '20,',\n",
              " 'card',\n",
              " 'document',\n",
              " 'lights',\n",
              " 'president',\n",
              " 'status',\n",
              " 'bcc:primary',\n",
              " 'berlind,',\n",
              " 'bob;',\n",
              " 'carraro,',\n",
              " 'cox,',\n",
              " 'culley,',\n",
              " 'dawson,',\n",
              " 'desel,',\n",
              " 'doris',\n",
              " 'femandez,',\n",
              " 'fox,',\n",
              " 'goss,',\n",
              " 'h.;',\n",
              " 'hanson,',\n",
              " 'ittermann,',\n",
              " 'jack;',\n",
              " 'james;',\n",
              " 'jim;',\n",
              " 'jun',\n",
              " 'keane,',\n",
              " 'l.ce:',\n",
              " 'litle',\n",
              " 'lynch,',\n",
              " 'm.subject:',\n",
              " 'maria;',\n",
              " 'mary;',\n",
              " 'michael;',\n",
              " 'patskan,',\n",
              " 'paula;',\n",
              " 'pmtor',\n",
              " 'podraza,',\n",
              " 'powers;',\n",
              " 'rob',\n",
              " 'susan;',\n",
              " 'tara;',\n",
              " 'tommy',\n",
              " 'tue,',\n",
              " 'urs;',\n",
              " 'v.;',\n",
              " 'wsa',\n",
              " '25,',\n",
              " 'good',\n",
              " 'asking',\n",
              " 'ron',\n",
              " 'jean',\n",
              " 'joe',\n",
              " 'me,',\n",
              " 'pos',\n",
              " 'request,',\n",
              " 'told',\n",
              " 'version',\n",
              " 'based',\n",
              " 'best',\n",
              " '19,',\n",
              " 'meetings',\n",
              " 'mentioned',\n",
              " 'needs',\n",
              " 'think',\n",
              " 'mr.',\n",
              " 'hee',\n",
              " 'ane',\n",
              " 'pee',\n",
              " 'ome',\n",
              " 'must',\n",
              " 'project',\n",
              " 'ale',\n",
              " 'vests',\n",
              " 'brand',\n",
              " '10,',\n",
              " 'board',\n",
              " 'yor',\n",
              " 'receive',\n",
              " 'ore',\n",
              " 'wee',\n",
              " 'aes',\n",
              " 'con',\n",
              " 'request',\n",
              " 'bey',\n",
              " 'dae',\n",
              " 'lhe',\n",
              " 'com',\n",
              " 'ond',\n",
              " 'ree',\n",
              " 'from',\n",
              " 'revised',\n",
              " 'ler',\n",
              " 'wet',\n",
              " 'co.',\n",
              " 'tet',\n",
              " 'reports',\n",
              " 'appreciate',\n",
              " 'het',\n",
              " 'pre',\n",
              " 'thank',\n",
              " 'bon',\n",
              " 'coon',\n",
              " '26,',\n",
              " 'file',\n",
              " 'bee',\n",
              " 'the.',\n",
              " 'thee',\n",
              " 'whe',\n",
              " 'way',\n",
              " 'soe',\n",
              " 'fay',\n",
              " 'ake',\n",
              " 'quality',\n",
              " 'was',\n",
              " 'fee',\n",
              " 'price',\n",
              " 'eal',\n",
              " 'moe',\n",
              " 'another',\n",
              " 'special',\n",
              " 'aed',\n",
              " 'ase',\n",
              " 'aud',\n",
              " 'bor',\n",
              " 'feo',\n",
              " 'pow',\n",
              " 'sat',\n",
              " 'avenuenew',\n",
              " 'truly',\n",
              " 'however,',\n",
              " 'regarding',\n",
              " 'mr,',\n",
              " 'committee',\n",
              " 'streetnew',\n",
              " 'visit',\n",
              " 'proposed',\n",
              " '17,',\n",
              " 'enclosing',\n",
              " 'progress',\n",
              " 'direct',\n",
              " 'members',\n",
              " 'using',\n",
              " 'type',\n",
              " 'discussed',\n",
              " 'four',\n",
              " 'grant',\n",
              " '10017dear',\n",
              " 'advisory',\n",
              " 'currently',\n",
              " 'executive',\n",
              " 'position',\n",
              " 'problems',\n",
              " 'total',\n",
              " 'include',\n",
              " 'development',\n",
              " 'fifth',\n",
              " 'services',\n",
              " 'that,',\n",
              " 'proposal',\n",
              " 'agreement',\n",
              " 'concern',\n",
              " '29,',\n",
              " 'expect',\n",
              " 'assistance',\n",
              " 'company,',\n",
              " 'policy',\n",
              " 'possible',\n",
              " 'ctr',\n",
              " 'd.c.',\n",
              " 'matter',\n",
              " 'united',\n",
              " 'included',\n",
              " 'preliminary',\n",
              " 'relative',\n",
              " '13,',\n",
              " 'event',\n",
              " 'william',\n",
              " 'great',\n",
              " 'requested',\n",
              " 'alpine',\n",
              " 'biomedical',\n",
              " 'patent',\n",
              " 'period',\n",
              " 'professor',\n",
              " 'major',\n",
              " 'comments',\n",
              " 'subject',\n",
              " 'copies',\n",
              " 'materials',\n",
              " 'receipt',\n",
              " 'require',\n",
              " 'effect',\n",
              " 'lorillard',\n",
              " 'manager',\n",
              " 'annual',\n",
              " 'products',\n",
              " '15,',\n",
              " 'informed',\n",
              " 'williamson',\n",
              " 'marketing',\n",
              " 'different',\n",
              " 'advise',\n",
              " 'life',\n",
              " '10022dear',\n",
              " '59th',\n",
              " 'advised',\n",
              " 'chicago,',\n",
              " 'enclose',\n",
              " 'saratoga',\n",
              " 'yorkdear',\n",
              " 'due',\n",
              " 'appropriate',\n",
              " 'relating',\n",
              " 'staff',\n",
              " '(212)',\n",
              " 'benson',\n",
              " 'fron',\n",
              " 'initial',\n",
              " 'outline',\n",
              " 'among',\n",
              " 'material',\n",
              " 'address',\n",
              " 'consumer',\n",
              " 'problem',\n",
              " 'account',\n",
              " 'continued',\n",
              " 'field',\n",
              " 'purpose',\n",
              " 'summary',\n",
              " 'come',\n",
              " 'action',\n",
              " 'companies',\n",
              " 'directly',\n",
              " 'future',\n",
              " 'sending',\n",
              " 'basic',\n",
              " 'director',\n",
              " 'behalf',\n",
              " 'brief',\n",
              " 'ms.',\n",
              " 'near',\n",
              " 'short',\n",
              " 'worked',\n",
              " 'accounts',\n",
              " 'applications',\n",
              " 'burnett',\n",
              " 'careful',\n",
              " 'established',\n",
              " 'expenses',\n",
              " 'hedges',\n",
              " 'inquiry',\n",
              " 'interoffice',\n",
              " 'investigation',\n",
              " 'madison',\n",
              " 'papers',\n",
              " 'reflect',\n",
              " 'renewal',\n",
              " 'trust',\n",
              " 'five',\n",
              " 'rate',\n",
              " 'show',\n",
              " 'system',\n",
              " 'upon',\n",
              " 'attention',\n",
              " 'conversation',\n",
              " 'corporate',\n",
              " 'know,',\n",
              " 'least',\n",
              " 'necessary',\n",
              " 'national',\n",
              " 'promotional',\n",
              " 'cambridge',\n",
              " 'inform',\n",
              " 'sheet']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvZ0Tk9c3gLd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "22f1f8ab-d1d3-4dfc-f0ea-29e8a095e318"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "maxlen = 200  # length\n",
        "max_words = 10000  # most common words\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "tokenizer_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(tokenizer_index))\n",
        "\n",
        "texts_train = asarray(tokenizer.texts_to_sequences(texts_train))\n",
        "texts_test = asarray(tokenizer.texts_to_sequences(texts_test))\n",
        "\n",
        "texts_train = pad_sequences(texts_train, maxlen=maxlen)\n",
        "texts_test = pad_sequences(texts_test, maxlen=maxlen)\n",
        "\n",
        "labels_train = asarray(labels_train)\n",
        "\n",
        "labels_test = asarray(labels_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 35511 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzmiy_Fyt76c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "# saving\n",
        "with open('/gdrive/My Drive/CDIP Dataset/cvl/Vocab/tokenizer_voc.pickle', 'wb') as tokenizer_vocab:\n",
        "  pickle.dump(tokenizer, tokenizer_vocab, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "# loading\n",
        "#with open('/gdrive/My Drive/Text_model_series/tokenizer_voc.pickle', 'rb') as tokenizer_vocab:\n",
        "  #  tokenizer = pickle.load(tokenizer_vocab)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr7khigN8PoY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ece63b89-7b2b-4148-aabc-9c0c0cc4834b"
      },
      "source": [
        "labels_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 4, 4, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lavYUNzUx-RW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "56e10929-3bb5-4dc9-9b34-e7d617512999"
      },
      "source": [
        "X_train, y_train = texts_train, labels_train\n",
        "X_test, y_test = texts_test, labels_test\n",
        "\n",
        "print('Shape of X_train tensor:', X_train.shape)\n",
        "print('Shape of y_train tensor:', y_train.shape)\n",
        "print('Shape of X_test tensor:', X_test.shape)\n",
        "print('Shape of y_test tensor:', y_test.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train tensor: (1310, 200)\n",
            "Shape of y_train tensor: (1310,)\n",
            "Shape of X_test tensor: (189, 200)\n",
            "Shape of y_test tensor: (189,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGos1PrP4y1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk16CLbZxjdW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a666d802-fee5-46fc-ff9e-89c217f41d2b"
      },
      "source": [
        "embedding_dim,embedding_matrix=load_fast_test_embeddings(max_words, best_feat_list, tokenizer_index)\n",
        "text_seq_input = Input(shape=(maxlen,), dtype='int32')\n",
        "text_embedding1 = Embedding(max_words, embedding_dim, input_length=maxlen,weights=[embedding_matrix], trainable=True)(text_seq_input)\n",
        "text_embedding2 = Embedding(max_words, embedding_dim, input_length=maxlen)(text_seq_input)\n",
        "text_embedding3 = Embedding(max_words, embedding_dim, input_length=maxlen)(text_seq_input)\n",
        "\n",
        "filter_sizes = [3, 5]\n",
        "conv_pools = []\n",
        "for text_embedding in [text_embedding1, text_embedding2, text_embedding3]:\n",
        "  for filter_size in filter_sizes:\n",
        "    l_zero = ZeroPadding1D((filter_size - 1, filter_size - 1))(text_embedding)\n",
        "    l_conv = Conv1D(filters=16, kernel_size=filter_size, padding='same', activation='tanh')(l_zero)\n",
        "    l_pool = GlobalMaxPool1D()(l_conv)\n",
        "    conv_pools.append(l_pool)\n",
        "\n",
        "l_merge = Concatenate(axis=1)(conv_pools)\n",
        "l_dense = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(l_merge)\n",
        "l_out = Dense(4, activation='softmax')(l_dense)\n",
        "model = Model(inputs=[text_seq_input], outputs=l_out)\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1000000 word vectors.\n",
            "comp 1008\n",
            "Model: \"functional_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 225)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_9 (Embedding)         (None, 225, 300)     3000000     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_10 (Embedding)        (None, 225, 300)     3000000     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_11 (Embedding)        (None, 225, 300)     3000000     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_18 (ZeroPadding1 (None, 229, 300)     0           embedding_9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_19 (ZeroPadding1 (None, 233, 300)     0           embedding_9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_20 (ZeroPadding1 (None, 229, 300)     0           embedding_10[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_21 (ZeroPadding1 (None, 233, 300)     0           embedding_10[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_22 (ZeroPadding1 (None, 229, 300)     0           embedding_11[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding1d_23 (ZeroPadding1 (None, 233, 300)     0           embedding_11[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_18 (Conv1D)              (None, 229, 16)      14416       zero_padding1d_18[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_19 (Conv1D)              (None, 233, 16)      24016       zero_padding1d_19[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_20 (Conv1D)              (None, 229, 16)      14416       zero_padding1d_20[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_21 (Conv1D)              (None, 233, 16)      24016       zero_padding1d_21[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_22 (Conv1D)              (None, 229, 16)      14416       zero_padding1d_22[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_23 (Conv1D)              (None, 233, 16)      24016       zero_padding1d_23[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_18 (Global (None, 16)           0           conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_19 (Global (None, 16)           0           conv1d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_20 (Global (None, 16)           0           conv1d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_21 (Global (None, 16)           0           conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_22 (Global (None, 16)           0           conv1d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_23 (Global (None, 16)           0           conv1d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 96)           0           global_max_pooling1d_18[0][0]    \n",
            "                                                                 global_max_pooling1d_19[0][0]    \n",
            "                                                                 global_max_pooling1d_20[0][0]    \n",
            "                                                                 global_max_pooling1d_21[0][0]    \n",
            "                                                                 global_max_pooling1d_22[0][0]    \n",
            "                                                                 global_max_pooling1d_23[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 128)          12416       concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 4)            516         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 9,128,228\n",
            "Trainable params: 9,128,228\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLycJAvM8-HN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81lRvYIBl4Ux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "1aab5f30-790d-43e0-96c9-3f80e62d7bd6"
      },
      "source": [
        "\n",
        "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(X_train, y_train,epochs=20,batch_size=50,validation_data=(X_test, y_test),verbose=1)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "27/27 [==============================] - 2s 65ms/step - loss: 1.9434 - accuracy: 0.5802 - val_loss: 1.4845 - val_accuracy: 0.7302\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 1s 38ms/step - loss: 1.1651 - accuracy: 0.8557 - val_loss: 1.0689 - val_accuracy: 0.8360\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 1s 38ms/step - loss: 0.7400 - accuracy: 0.9443 - val_loss: 0.8346 - val_accuracy: 0.8571\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 1s 38ms/step - loss: 0.4774 - accuracy: 0.9740 - val_loss: 0.6662 - val_accuracy: 0.8571\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 1s 37ms/step - loss: 0.3118 - accuracy: 0.9832 - val_loss: 0.5584 - val_accuracy: 0.8677\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 1s 38ms/step - loss: 0.2031 - accuracy: 0.9878 - val_loss: 0.6376 - val_accuracy: 0.7937\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 1s 37ms/step - loss: 0.1441 - accuracy: 0.9847 - val_loss: 0.4751 - val_accuracy: 0.8519\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 1s 37ms/step - loss: 0.1092 - accuracy: 0.9832 - val_loss: 0.5027 - val_accuracy: 0.8519\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 1s 38ms/step - loss: 0.0881 - accuracy: 0.9847 - val_loss: 0.4295 - val_accuracy: 0.8677\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 1s 38ms/step - loss: 0.0740 - accuracy: 0.9893 - val_loss: 0.4366 - val_accuracy: 0.8519\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 1s 37ms/step - loss: 0.0654 - accuracy: 0.9863 - val_loss: 0.4929 - val_accuracy: 0.8466\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 1s 37ms/step - loss: 0.0599 - accuracy: 0.9847 - val_loss: 0.4141 - val_accuracy: 0.8519\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 1s 37ms/step - loss: 0.0540 - accuracy: 0.9885 - val_loss: 0.5546 - val_accuracy: 0.7989\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 1s 37ms/step - loss: 0.0487 - accuracy: 0.9878 - val_loss: 0.4057 - val_accuracy: 0.8677\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 1s 37ms/step - loss: 0.0432 - accuracy: 0.9893 - val_loss: 0.4875 - val_accuracy: 0.8571\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 1s 37ms/step - loss: 0.0449 - accuracy: 0.9863 - val_loss: 0.4195 - val_accuracy: 0.8413\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 1s 36ms/step - loss: 0.0417 - accuracy: 0.9855 - val_loss: 0.4534 - val_accuracy: 0.8519\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 1s 38ms/step - loss: 0.0375 - accuracy: 0.9901 - val_loss: 0.5450 - val_accuracy: 0.8360\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 1s 37ms/step - loss: 0.0380 - accuracy: 0.9863 - val_loss: 0.4470 - val_accuracy: 0.8519\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 1s 38ms/step - loss: 0.0366 - accuracy: 0.9870 - val_loss: 0.4608 - val_accuracy: 0.8519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4aa965ae10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PngJl9n2n9f4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/gdrive/My Drive/CDIP Dataset/cvl/text_model_cvl_cdip.hdf5')"
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}