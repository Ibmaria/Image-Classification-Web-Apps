{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Inek_uNVVZ9q"
   },
   "outputs": [],
   "source": [
    "[1]*10**10 #get more ram quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "B1NBujP0V-P0",
    "outputId": "c3a1c013-2c7a-474e-e549-e0aed2a1bd9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n",
      "/gdrive/My Drive/CDIP Dataset\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    " \n",
    "drive.mount('/gdrive')\n",
    "# the project's folder\n",
    "%cd /gdrive/'My Drive'/CDIP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Et3ug_yLWf06",
    "outputId": "3e6d20ca-f36a-4e6b-c7ba-ac85d14f5d47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#I unzip images folder\n",
    "from zipfile import ZipFile\n",
    "filename='test_data.zip'\n",
    "with ZipFile(filename,'r') as zip:\n",
    "  zip.extractall()\n",
    "  print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "0tTXDehpTWie",
    "outputId": "b820da2b-3b37-44a7-c46f-6a8991f639d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "00kGR2t6Y-qF",
    "outputId": "e44b7bfb-598a-4351-b967-005cd0aa8a7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score,f1_score, recall_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, Dense, Input, Conv1D, MaxPool1D, Concatenate, Flatten, Dropout\n",
    "from keras.layers import ZeroPadding1D, Activation, GlobalMaxPool1D\n",
    "from keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MRDZP21ZaAzt"
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "olbFB1YKZU71"
   },
   "outputs": [],
   "source": [
    "def load_fast_test_embeddings(max_words, best_feat_list, tokenizer_index):\n",
    "    f = open('/gdrive/My Drive/CDIP Dataset/cvl/Fast_text-emb/wiki-news-300d-1M.vec')\n",
    "    embeddings_index = {}\n",
    "    for line in f:\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      coefs =asarray(values[1:], dtype='float32')\n",
    "      embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    embedding_dim = 300 # fastext dim \n",
    "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "    for word, i in tokenizer_index.items():\n",
    "      if i < max_words and word in best_feat_list:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "          embedding_matrix[i] = embedding_vector\n",
    "    return embedding_dim, embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3kP4QOc_epsn"
   },
   "outputs": [],
   "source": [
    "ensemble_labels = ['Adver','email','handwriten','letter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "x3103wpQZ-75",
    "outputId": "476d78a2-3405-439c-e7bf-b6761b79c88a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gdrive/My Drive/CDIP Dataset/cvl/train_data/Adver\n",
      "/gdrive/My Drive/CDIP Dataset/cvl/train_data/email\n",
      "/gdrive/My Drive/CDIP Dataset/cvl/train_data/handwriten\n",
      "/gdrive/My Drive/CDIP Dataset/cvl/train_data/letter\n",
      "Length of Training Text: 1310\n",
      "Length of  Training Labels: 1310\n"
     ]
    }
   ],
   "source": [
    "#for training data\n",
    "texts_train = []\n",
    "labels_train = []\n",
    "not_read=[]\n",
    "include_extension =['txt']\n",
    "#train_dir = os.path.join(Dataset_Dir,'train')\n",
    "for label_index, label_name in enumerate(ensemble_labels):\n",
    "  base = '/gdrive/My Drive/CDIP Dataset/cvl/train_data/'+label_name\n",
    "\n",
    "  #text files selection\n",
    "  files = [fn for fn in os.listdir(base) if any(fn.endswith(ext) for ext in include_extension)]\n",
    "  for  file in files:\n",
    "   \n",
    "    fr = codecs.open(base+'/'+file,encoding='utf-8',errors='ignore')\n",
    "    \n",
    "    text =fr.read().replace('\\n','')\n",
    "\n",
    "    \n",
    "    texts_train.append(text)\n",
    "    labels_train.append(label_index)\n",
    "    fr.close()\n",
    "      \n",
    "           \n",
    "print('Length of Training Text: %s' % len(texts_train))\n",
    "print('Length of  Training Labels: %s' % len(labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8OKshHqdR5Du",
    "outputId": "5a0a77c7-dcd5-493d-ba7b-61f0c097a4d1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'last hunt, major hooumsmoked cigarotte stamped withhis family crest,now everybody stamped ownfamily crest, .walmost then, try tobe.)'"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bonxwAXBiRbr",
    "outputId": "04bd9538-cd81-4a1a-ad64-59841c1c9f65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Test Text: 189\n",
      "Length of  Test Labels: 189\n"
     ]
    }
   ],
   "source": [
    "#for training data\n",
    "texts_test = []\n",
    "labels_test = []\n",
    "include_extension =['txt']\n",
    "\n",
    "for label_index, label_name in enumerate(ensemble_labels):\n",
    "  base = '/gdrive/My Drive/CDIP Dataset/cvl/test_data/'+label_name\n",
    "  #text files selection\n",
    "  files = [fn for fn in os.listdir(base) if any(fn.endswith(ext) for ext in include_extension)]\n",
    "  for  file in files:\n",
    "    fr = codecs.open(base+'/'+file,encoding='utf-8',errors='ignore')\n",
    "    texts_test.append(fr.read().replace('\\n', ''))\n",
    "    labels_test.append(label_index)\n",
    "    fr.close()\n",
    "      \n",
    "           \n",
    "print('Length of Test Text: %s' % len(texts_test))\n",
    "print('Length of  Test Labels: %s' % len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "DsKBOdLnywJS",
    "outputId": "7aacb5df-9cd8-4bb7-b32e-646bb89cd0fc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"the new lookingold# and general's warnin' jokin:causes lung cancer, hear ase,emphy and may complicate empane)\""
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_test[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cRUzCwxmu1Ak"
   },
   "outputs": [],
   "source": [
    "corpus = texts_train + texts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SCKIlJMHvnrT"
   },
   "outputs": [],
   "source": [
    "# select n features from each class\n",
    "number_of_features_each_class=500\n",
    "best_feat_list=list()\n",
    "path_results_features_selection = '/gdrive/My Drive/CDIP Dataset/cvl/best_features/'\n",
    "ensemble_labels = ['Adver','email','handwriten','letter']\n",
    "for label in ensemble_labels:\n",
    "  current_label=path_results_features_selection+label\n",
    "  filename=label+'.txt'\n",
    "  fr = codecs.open(current_label+'/'+filename,encoding='utf-8',errors='ignore')\n",
    "  best_features=fr.read().split()\n",
    "  for feature in range(0, number_of_features_each_class):\n",
    "    if feature > len(best_features) - 1:\n",
    "      break\n",
    "    if best_features[feature] not in best_feat_list:\n",
    "      best_feat_list.append(best_features[feature])\n",
    "  #select feature for label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KlsSc4JgNhY3",
    "outputId": "a2c0fe42-b14e-46d8-8bfe-4e31cd7944fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['would',\n",
       " 'please',\n",
       " 'surgeon',\n",
       " \"general's\",\n",
       " 'warning:',\n",
       " 'know',\n",
       " 'and',\n",
       " 'york,',\n",
       " 'dr.',\n",
       " 'low',\n",
       " 'meeting',\n",
       " 'your',\n",
       " 'has',\n",
       " 'also',\n",
       " 'modified',\n",
       " 'information',\n",
       " 'original',\n",
       " 'smoking',\n",
       " 'new',\n",
       " 'work',\n",
       " 'tobacco',\n",
       " 'cigarette',\n",
       " 'product',\n",
       " 'research',\n",
       " 'copy',\n",
       " 'lung',\n",
       " 'york',\n",
       " 'let',\n",
       " 'forward',\n",
       " 'need',\n",
       " 'see',\n",
       " 'taste',\n",
       " 'october',\n",
       " 'time',\n",
       " 'following',\n",
       " 'january',\n",
       " 'two',\n",
       " 'women',\n",
       " 'june',\n",
       " 'next',\n",
       " 'dangerous',\n",
       " 'park',\n",
       " 'you',\n",
       " 'august',\n",
       " 'pregnant',\n",
       " 'september',\n",
       " 'hope',\n",
       " 'this',\n",
       " 'provide',\n",
       " 'call',\n",
       " 'july',\n",
       " 'february',\n",
       " 'received',\n",
       " 'tar',\n",
       " 'dear',\n",
       " 'use',\n",
       " 'find',\n",
       " 'march',\n",
       " 'like',\n",
       " 'heart',\n",
       " 'april',\n",
       " 'make',\n",
       " 'general',\n",
       " 'could',\n",
       " 'flavor',\n",
       " 'contains',\n",
       " 'list',\n",
       " 'number',\n",
       " 'office',\n",
       " 'end',\n",
       " 'november',\n",
       " 'smoke',\n",
       " 'well',\n",
       " 'give',\n",
       " 'birth',\n",
       " 'complicate',\n",
       " 'premature',\n",
       " 'might',\n",
       " 'eee',\n",
       " 'box',\n",
       " 'additional',\n",
       " 'interest',\n",
       " 'serious',\n",
       " 'week',\n",
       " 'last',\n",
       " 'current',\n",
       " 'recent',\n",
       " 'within',\n",
       " 'carbon',\n",
       " 'december',\n",
       " 'reduces',\n",
       " 'program',\n",
       " 'year',\n",
       " 'east',\n",
       " 'birth,',\n",
       " 'cancer,',\n",
       " 'the',\n",
       " 'no.',\n",
       " 'working',\n",
       " 'north',\n",
       " 'support',\n",
       " 'company',\n",
       " 'several',\n",
       " 'look',\n",
       " 'result',\n",
       " 'camels',\n",
       " 'quitting',\n",
       " '22,',\n",
       " 'able',\n",
       " 'date',\n",
       " 'asked',\n",
       " 'fetalinjury,',\n",
       " 'smokingby',\n",
       " '23,',\n",
       " '24,',\n",
       " 'ofthe',\n",
       " 'not',\n",
       " 'questions',\n",
       " 'used',\n",
       " 'philip',\n",
       " 'first',\n",
       " 'avenue,',\n",
       " 'check',\n",
       " 'soon',\n",
       " 'smokers',\n",
       " 'fic',\n",
       " 'only',\n",
       " 'risks',\n",
       " 'weight.',\n",
       " 'carolina',\n",
       " 'business',\n",
       " 'present',\n",
       " 'market',\n",
       " 'state',\n",
       " 'ina',\n",
       " 'for',\n",
       " 'page',\n",
       " 'toyour',\n",
       " 'held',\n",
       " 'provided',\n",
       " 'results',\n",
       " 'scheduled',\n",
       " 'advertising',\n",
       " 'day',\n",
       " 'every',\n",
       " 'mg.',\n",
       " 'continue',\n",
       " 'final',\n",
       " 'media',\n",
       " 'states',\n",
       " 'menthol',\n",
       " 'may',\n",
       " 'conference',\n",
       " 'response',\n",
       " 'scientific',\n",
       " 'free',\n",
       " 'greatly',\n",
       " 'world',\n",
       " 'help',\n",
       " 'pregnancy.',\n",
       " 'smokingnow',\n",
       " 'data',\n",
       " 'pleased',\n",
       " 'team',\n",
       " 'whether',\n",
       " 'try',\n",
       " 'public',\n",
       " 'right',\n",
       " 'already',\n",
       " 'young',\n",
       " 'health.',\n",
       " 'concerning',\n",
       " 'done',\n",
       " 'second',\n",
       " 'tom',\n",
       " 'change',\n",
       " 'issue',\n",
       " 'opportunity',\n",
       " 'but',\n",
       " 'site',\n",
       " 'group',\n",
       " 'slims',\n",
       " 'tat',\n",
       " 'since',\n",
       " 'cigarette,',\n",
       " 'parents',\n",
       " 'ultra',\n",
       " 'smokingcauses',\n",
       " 'sugeon',\n",
       " '16,',\n",
       " '27,',\n",
       " 'fax',\n",
       " 'including',\n",
       " 'study',\n",
       " 'early',\n",
       " 'process',\n",
       " 'set',\n",
       " 'camel',\n",
       " 'made',\n",
       " 'important',\n",
       " 'buy',\n",
       " 'children',\n",
       " 'enjoy',\n",
       " 'factor',\n",
       " 'king',\n",
       " 'monoxide.',\n",
       " 'peer',\n",
       " 'area',\n",
       " 'fact',\n",
       " 'key',\n",
       " 'past',\n",
       " 'test',\n",
       " 'reynolds',\n",
       " 'filter',\n",
       " 'american',\n",
       " 'three',\n",
       " 'kids',\n",
       " 'today.',\n",
       " 'ftc',\n",
       " '18,',\n",
       " 'center',\n",
       " 'certain',\n",
       " 'division',\n",
       " 'doral',\n",
       " 'place',\n",
       " 'cigarettes',\n",
       " 'take',\n",
       " 'say',\n",
       " 'name',\n",
       " 'today',\n",
       " 'west',\n",
       " 'any',\n",
       " 'means',\n",
       " 'cut',\n",
       " 'avenue',\n",
       " 'consider',\n",
       " 'contain',\n",
       " 'control',\n",
       " 'law',\n",
       " 'specific',\n",
       " 'street,',\n",
       " 'thought',\n",
       " 'to:',\n",
       " 'wish',\n",
       " 'report',\n",
       " 'many',\n",
       " 'america’s',\n",
       " 'cigarette!',\n",
       " 'don’t',\n",
       " 'fte',\n",
       " 'heath,',\n",
       " 'ing:',\n",
       " 'waring:',\n",
       " '‘surgeon',\n",
       " 'feel',\n",
       " \"i'm\",\n",
       " 'paper',\n",
       " 'sample',\n",
       " 'together',\n",
       " 'issues',\n",
       " 'all',\n",
       " 'order',\n",
       " 'sure',\n",
       " 'because',\n",
       " 'advertisement',\n",
       " 'smoking,',\n",
       " 'changes',\n",
       " 'frank',\n",
       " 'looking',\n",
       " 'management',\n",
       " 'programs',\n",
       " 'required',\n",
       " 'samples',\n",
       " 'various',\n",
       " 'very',\n",
       " 'famous',\n",
       " 'kings',\n",
       " 'men',\n",
       " 'method.',\n",
       " 'that’s',\n",
       " 'then',\n",
       " '28,',\n",
       " 'join',\n",
       " 'seen',\n",
       " 'our',\n",
       " 'health',\n",
       " 'bring',\n",
       " 'ate',\n",
       " 'available',\n",
       " 'que',\n",
       " 'analysis',\n",
       " 'level',\n",
       " 'line',\n",
       " 'potential',\n",
       " 'service',\n",
       " 'choice',\n",
       " 'ese',\n",
       " 'gives',\n",
       " 'helping',\n",
       " 'hon',\n",
       " \"it's\",\n",
       " 'leading',\n",
       " 'sense',\n",
       " 'tol',\n",
       " 'ask',\n",
       " 'cover',\n",
       " 'statement',\n",
       " 'meet',\n",
       " 'third',\n",
       " 'even',\n",
       " 'got',\n",
       " 'wil',\n",
       " 'never',\n",
       " 'morris',\n",
       " 'want',\n",
       " 'agency',\n",
       " 'approval',\n",
       " 'decision',\n",
       " 'develop',\n",
       " 'idea',\n",
       " 'latest',\n",
       " 'prior',\n",
       " 'probably',\n",
       " 'street',\n",
       " 'suggested',\n",
       " 'yet',\n",
       " 'winston',\n",
       " 'alot',\n",
       " 'drug',\n",
       " 'son',\n",
       " \"that's\",\n",
       " 'aware',\n",
       " 'making',\n",
       " 'medical',\n",
       " 'merit',\n",
       " 'production',\n",
       " 'question',\n",
       " 'room',\n",
       " \"100's\",\n",
       " 'appendix',\n",
       " 'cigaete',\n",
       " 'ea,',\n",
       " 'experts',\n",
       " 'filter.',\n",
       " 'introducing',\n",
       " 'laser',\n",
       " 'mater',\n",
       " 'nose',\n",
       " 'people.',\n",
       " 'straight',\n",
       " 'tenet',\n",
       " 'totally',\n",
       " 'window',\n",
       " 'air',\n",
       " 'along',\n",
       " 'complete',\n",
       " 'full',\n",
       " 'legal',\n",
       " 'sales',\n",
       " 'form',\n",
       " 'found',\n",
       " 'web',\n",
       " 'brands',\n",
       " 'regular',\n",
       " 'tae',\n",
       " 'also,',\n",
       " 'costs',\n",
       " 'developed',\n",
       " 'effects',\n",
       " 'jan',\n",
       " 'main',\n",
       " 'n.y.',\n",
       " 'seems',\n",
       " 'stated',\n",
       " 'time,',\n",
       " 'ape',\n",
       " 'certainly',\n",
       " 'family',\n",
       " 'friends',\n",
       " 'true',\n",
       " 'me.',\n",
       " 'suite',\n",
       " 'much',\n",
       " 'per',\n",
       " 'advice',\n",
       " 'ave.',\n",
       " 'be.',\n",
       " 'bith',\n",
       " 'bow',\n",
       " 'child',\n",
       " 'choices',\n",
       " 'cool',\n",
       " 'ee:',\n",
       " 'goodness',\n",
       " 'happened',\n",
       " 'heath',\n",
       " 'lowest',\n",
       " 'natural',\n",
       " 'ofall',\n",
       " 'opt',\n",
       " 'po.',\n",
       " 'smoked',\n",
       " 'sree',\n",
       " 'stop',\n",
       " 'than',\n",
       " \"there's\",\n",
       " \"they're\",\n",
       " 'throat',\n",
       " 'timn',\n",
       " 'virgina',\n",
       " 'yes,',\n",
       " 'cost',\n",
       " 'forthe',\n",
       " 'without',\n",
       " 'case',\n",
       " 'believe',\n",
       " 'ever',\n",
       " 'amount',\n",
       " 'basis',\n",
       " 'begin',\n",
       " 'cannot',\n",
       " 'che',\n",
       " 'clear',\n",
       " 'code',\n",
       " 'doctor',\n",
       " 'efforts',\n",
       " 'environmental',\n",
       " 'existing',\n",
       " 'given',\n",
       " 'inc.',\n",
       " 'indicated',\n",
       " 'ken',\n",
       " 'print',\n",
       " 'related',\n",
       " 'section',\n",
       " 'separate',\n",
       " 'something',\n",
       " 'standard',\n",
       " 'testing',\n",
       " 'tha',\n",
       " 'weeks',\n",
       " 'what',\n",
       " 'white',\n",
       " 'industry',\n",
       " 'people',\n",
       " 'with',\n",
       " 'example,',\n",
       " 'ine',\n",
       " 'red',\n",
       " 'should',\n",
       " 'from:',\n",
       " 're:',\n",
       " 'date:',\n",
       " 'touched',\n",
       " 'pmto:',\n",
       " 'date:sent',\n",
       " 'monday,',\n",
       " 'fw:',\n",
       " 'karen',\n",
       " 'amlast',\n",
       " 'amto:',\n",
       " 'wednesday,',\n",
       " 'pmlast',\n",
       " 'friday,',\n",
       " '2001-nov-20',\n",
       " 'daragan,',\n",
       " 'michael',\n",
       " 'get',\n",
       " 'attached',\n",
       " 'thursday,',\n",
       " 'message—from:',\n",
       " 'john',\n",
       " 'david',\n",
       " 'l.;',\n",
       " 'mike',\n",
       " 'tuesday,',\n",
       " 'w.;',\n",
       " 'henry',\n",
       " 'smith,',\n",
       " 'mary',\n",
       " 'rick',\n",
       " 'message',\n",
       " 'sent',\n",
       " 'john;',\n",
       " 'messagefrom:',\n",
       " 'carolyn',\n",
       " 'p.;',\n",
       " 's.;',\n",
       " 'discuss',\n",
       " 'ysp',\n",
       " '08,',\n",
       " 'j.;',\n",
       " 'm.;',\n",
       " 'mark',\n",
       " 'prevention',\n",
       " 'jim',\n",
       " 'a.;',\n",
       " 'greg',\n",
       " 'judy',\n",
       " 'mccormick,',\n",
       " 'orginal',\n",
       " 'going',\n",
       " 'roger',\n",
       " 'letter',\n",
       " 'back',\n",
       " 'datsent',\n",
       " 'file:',\n",
       " 'kevin',\n",
       " 'linda',\n",
       " 'kelly',\n",
       " 'levy,',\n",
       " 'draft',\n",
       " 'attachments:',\n",
       " 'brendan',\n",
       " 'd.;',\n",
       " 'merlo,',\n",
       " 'mike;',\n",
       " 'tomorrow',\n",
       " 'review',\n",
       " 'confidential',\n",
       " 'robert',\n",
       " 'denise',\n",
       " 'dave',\n",
       " 'friday',\n",
       " 'nancy',\n",
       " 'schedule',\n",
       " '07,',\n",
       " 'carol',\n",
       " 'chaikin,',\n",
       " 'datedate:',\n",
       " 'f.;',\n",
       " 'karen;',\n",
       " 'nelson,',\n",
       " 'sent:',\n",
       " 'threatened',\n",
       " '‘thursday,',\n",
       " 'anything',\n",
       " 'richard',\n",
       " '03,',\n",
       " 'ellen',\n",
       " 'ongoing',\n",
       " 'send',\n",
       " 'connection',\n",
       " 'bill',\n",
       " 'contact',\n",
       " 'pat',\n",
       " 'thanks',\n",
       " 'council',\n",
       " 'b.;',\n",
       " 'david;',\n",
       " 'defense,',\n",
       " 'update',\n",
       " 'note',\n",
       " 'joint',\n",
       " 'richmond',\n",
       " 'enclosed',\n",
       " 'product,',\n",
       " 'virginia',\n",
       " 'years',\n",
       " 'thomas',\n",
       " 'e-mail',\n",
       " 'just',\n",
       " 'barbara',\n",
       " 'bruce',\n",
       " 'else',\n",
       " 'susan',\n",
       " '11,',\n",
       " 'e.;',\n",
       " 'g.;',\n",
       " 'solana,',\n",
       " 'spoke',\n",
       " 'wed,',\n",
       " 'application',\n",
       " 'said',\n",
       " 'moore,',\n",
       " 'one',\n",
       " 'morning',\n",
       " 'pmusa',\n",
       " 'mail',\n",
       " 'news',\n",
       " 'george',\n",
       " '‘the',\n",
       " 'ike',\n",
       " 'someone',\n",
       " 'steve',\n",
       " 'a.subject:',\n",
       " 'afternoon',\n",
       " 'amsubject:',\n",
       " 'amtor',\n",
       " 'c.;',\n",
       " 'davis,',\n",
       " 'ellen;',\n",
       " 'message:from:',\n",
       " 'pmsubject:',\n",
       " 'privileged',\n",
       " 'robin',\n",
       " 'sanders,',\n",
       " 'timothy',\n",
       " 'institute',\n",
       " 'affairs',\n",
       " 'subject:',\n",
       " 'tee',\n",
       " 'youth',\n",
       " 'plan',\n",
       " 'nicotine',\n",
       " 'wanted',\n",
       " 'monday',\n",
       " 'attend',\n",
       " 'donald',\n",
       " '14,',\n",
       " 'telephone',\n",
       " 'them.',\n",
       " 'dates',\n",
       " 'move',\n",
       " 'updated',\n",
       " 'vice',\n",
       " 'via',\n",
       " 'cancer',\n",
       " 'university',\n",
       " '01,',\n",
       " '05,',\n",
       " 'albert;',\n",
       " 'attachments',\n",
       " 'brown,',\n",
       " 'byron',\n",
       " 'cheryl',\n",
       " 'email',\n",
       " 'frank;',\n",
       " 'importance:',\n",
       " 'message-from:',\n",
       " 'meyne,',\n",
       " 'n.;',\n",
       " 'nicoli,',\n",
       " 'pfeil,',\n",
       " 'pmte',\n",
       " 'r.;',\n",
       " 'ryan,',\n",
       " 'scruggs,',\n",
       " 't.;',\n",
       " 'walk,',\n",
       " 'whidden,',\n",
       " 'williams,',\n",
       " 'apr',\n",
       " 'inthe',\n",
       " 'high',\n",
       " 'opinion',\n",
       " 'presentation',\n",
       " 'craig',\n",
       " 'eric',\n",
       " 'jones,',\n",
       " 'scott',\n",
       " 'department',\n",
       " 'net',\n",
       " 'these',\n",
       " 'put',\n",
       " 'bob',\n",
       " 'employee',\n",
       " 'looks',\n",
       " 'objectives',\n",
       " '20,',\n",
       " 'card',\n",
       " 'document',\n",
       " 'lights',\n",
       " 'president',\n",
       " 'status',\n",
       " 'bcc:primary',\n",
       " 'berlind,',\n",
       " 'bob;',\n",
       " 'carraro,',\n",
       " 'cox,',\n",
       " 'culley,',\n",
       " 'dawson,',\n",
       " 'desel,',\n",
       " 'doris',\n",
       " 'femandez,',\n",
       " 'fox,',\n",
       " 'goss,',\n",
       " 'h.;',\n",
       " 'hanson,',\n",
       " 'ittermann,',\n",
       " 'jack;',\n",
       " 'james;',\n",
       " 'jim;',\n",
       " 'jun',\n",
       " 'keane,',\n",
       " 'l.ce:',\n",
       " 'litle',\n",
       " 'lynch,',\n",
       " 'm.subject:',\n",
       " 'maria;',\n",
       " 'mary;',\n",
       " 'michael;',\n",
       " 'patskan,',\n",
       " 'paula;',\n",
       " 'pmtor',\n",
       " 'podraza,',\n",
       " 'powers;',\n",
       " 'rob',\n",
       " 'susan;',\n",
       " 'tara;',\n",
       " 'tommy',\n",
       " 'tue,',\n",
       " 'urs;',\n",
       " 'v.;',\n",
       " 'wsa',\n",
       " '25,',\n",
       " 'good',\n",
       " 'asking',\n",
       " 'ron',\n",
       " 'jean',\n",
       " 'joe',\n",
       " 'me,',\n",
       " 'pos',\n",
       " 'request,',\n",
       " 'told',\n",
       " 'version',\n",
       " 'based',\n",
       " 'best',\n",
       " '19,',\n",
       " 'meetings',\n",
       " 'mentioned',\n",
       " 'needs',\n",
       " 'think',\n",
       " 'mr.',\n",
       " 'hee',\n",
       " 'ane',\n",
       " 'pee',\n",
       " 'ome',\n",
       " 'must',\n",
       " 'project',\n",
       " 'ale',\n",
       " 'vests',\n",
       " 'brand',\n",
       " '10,',\n",
       " 'board',\n",
       " 'yor',\n",
       " 'receive',\n",
       " 'ore',\n",
       " 'wee',\n",
       " 'aes',\n",
       " 'con',\n",
       " 'request',\n",
       " 'bey',\n",
       " 'dae',\n",
       " 'lhe',\n",
       " 'com',\n",
       " 'ond',\n",
       " 'ree',\n",
       " 'from',\n",
       " 'revised',\n",
       " 'ler',\n",
       " 'wet',\n",
       " 'co.',\n",
       " 'tet',\n",
       " 'reports',\n",
       " 'appreciate',\n",
       " 'het',\n",
       " 'pre',\n",
       " 'thank',\n",
       " 'bon',\n",
       " 'coon',\n",
       " '26,',\n",
       " 'file',\n",
       " 'bee',\n",
       " 'the.',\n",
       " 'thee',\n",
       " 'whe',\n",
       " 'way',\n",
       " 'soe',\n",
       " 'fay',\n",
       " 'ake',\n",
       " 'quality',\n",
       " 'was',\n",
       " 'fee',\n",
       " 'price',\n",
       " 'eal',\n",
       " 'moe',\n",
       " 'another',\n",
       " 'special',\n",
       " 'aed',\n",
       " 'ase',\n",
       " 'aud',\n",
       " 'bor',\n",
       " 'feo',\n",
       " 'pow',\n",
       " 'sat',\n",
       " 'avenuenew',\n",
       " 'truly',\n",
       " 'however,',\n",
       " 'regarding',\n",
       " 'mr,',\n",
       " 'committee',\n",
       " 'streetnew',\n",
       " 'visit',\n",
       " 'proposed',\n",
       " '17,',\n",
       " 'enclosing',\n",
       " 'progress',\n",
       " 'direct',\n",
       " 'members',\n",
       " 'using',\n",
       " 'type',\n",
       " 'discussed',\n",
       " 'four',\n",
       " 'grant',\n",
       " '10017dear',\n",
       " 'advisory',\n",
       " 'currently',\n",
       " 'executive',\n",
       " 'position',\n",
       " 'problems',\n",
       " 'total',\n",
       " 'include',\n",
       " 'development',\n",
       " 'fifth',\n",
       " 'services',\n",
       " 'that,',\n",
       " 'proposal',\n",
       " 'agreement',\n",
       " 'concern',\n",
       " '29,',\n",
       " 'expect',\n",
       " 'assistance',\n",
       " 'company,',\n",
       " 'policy',\n",
       " 'possible',\n",
       " 'ctr',\n",
       " 'd.c.',\n",
       " 'matter',\n",
       " 'united',\n",
       " 'included',\n",
       " 'preliminary',\n",
       " 'relative',\n",
       " '13,',\n",
       " 'event',\n",
       " 'william',\n",
       " 'great',\n",
       " 'requested',\n",
       " 'alpine',\n",
       " 'biomedical',\n",
       " 'patent',\n",
       " 'period',\n",
       " 'professor',\n",
       " 'major',\n",
       " 'comments',\n",
       " 'subject',\n",
       " 'copies',\n",
       " 'materials',\n",
       " 'receipt',\n",
       " 'require',\n",
       " 'effect',\n",
       " 'lorillard',\n",
       " 'manager',\n",
       " 'annual',\n",
       " 'products',\n",
       " '15,',\n",
       " 'informed',\n",
       " 'williamson',\n",
       " 'marketing',\n",
       " 'different',\n",
       " 'advise',\n",
       " 'life',\n",
       " '10022dear',\n",
       " '59th',\n",
       " 'advised',\n",
       " 'chicago,',\n",
       " 'enclose',\n",
       " 'saratoga',\n",
       " 'yorkdear',\n",
       " 'due',\n",
       " 'appropriate',\n",
       " 'relating',\n",
       " 'staff',\n",
       " '(212)',\n",
       " 'benson',\n",
       " 'fron',\n",
       " 'initial',\n",
       " 'outline',\n",
       " 'among',\n",
       " 'material',\n",
       " 'address',\n",
       " 'consumer',\n",
       " 'problem',\n",
       " 'account',\n",
       " 'continued',\n",
       " 'field',\n",
       " 'purpose',\n",
       " 'summary',\n",
       " 'come',\n",
       " 'action',\n",
       " 'companies',\n",
       " 'directly',\n",
       " 'future',\n",
       " 'sending',\n",
       " 'basic',\n",
       " 'director',\n",
       " 'behalf',\n",
       " 'brief',\n",
       " 'ms.',\n",
       " 'near',\n",
       " 'short',\n",
       " 'worked',\n",
       " 'accounts',\n",
       " 'applications',\n",
       " 'burnett',\n",
       " 'careful',\n",
       " 'established',\n",
       " 'expenses',\n",
       " 'hedges',\n",
       " 'inquiry',\n",
       " 'interoffice',\n",
       " 'investigation',\n",
       " 'madison',\n",
       " 'papers',\n",
       " 'reflect',\n",
       " 'renewal',\n",
       " 'trust',\n",
       " 'five',\n",
       " 'rate',\n",
       " 'show',\n",
       " 'system',\n",
       " 'upon',\n",
       " 'attention',\n",
       " 'conversation',\n",
       " 'corporate',\n",
       " 'know,',\n",
       " 'least',\n",
       " 'necessary',\n",
       " 'national',\n",
       " 'promotional',\n",
       " 'cambridge',\n",
       " 'inform',\n",
       " 'sheet']"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nvZ0Tk9c3gLd",
    "outputId": "22f1f8ab-d1d3-4dfc-f0ea-29e8a095e318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35511 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = 200  # length\n",
    "max_words = 10000  # most common words\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "tokenizer_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(tokenizer_index))\n",
    "\n",
    "texts_train = asarray(tokenizer.texts_to_sequences(texts_train))\n",
    "texts_test = asarray(tokenizer.texts_to_sequences(texts_test))\n",
    "\n",
    "texts_train = pad_sequences(texts_train, maxlen=maxlen)\n",
    "texts_test = pad_sequences(texts_test, maxlen=maxlen)\n",
    "\n",
    "labels_train = asarray(labels_train)\n",
    "\n",
    "labels_test = asarray(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lzmiy_Fyt76c"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('/gdrive/My Drive/CDIP Dataset/cvl/Vocab/tokenizer_voc.pickle', 'wb') as tokenizer_vocab:\n",
    "  pickle.dump(tokenizer, tokenizer_vocab, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# loading\n",
    "#with open('/gdrive/My Drive/Text_model_series/tokenizer_voc.pickle', 'rb') as tokenizer_vocab:\n",
    "  #  tokenizer = pickle.load(tokenizer_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cr7khigN8PoY",
    "outputId": "ece63b89-7b2b-4148-aabc-9c0c0cc4834b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "lavYUNzUx-RW",
    "outputId": "56e10929-3bb5-4dc9-9b34-e7d617512999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train tensor: (1310, 200)\n",
      "Shape of y_train tensor: (1310,)\n",
      "Shape of X_test tensor: (189, 200)\n",
      "Shape of y_test tensor: (189,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = texts_train, labels_train\n",
    "X_test, y_test = texts_test, labels_test\n",
    "\n",
    "print('Shape of X_train tensor:', X_train.shape)\n",
    "print('Shape of y_train tensor:', y_train.shape)\n",
    "print('Shape of X_test tensor:', X_test.shape)\n",
    "print('Shape of y_test tensor:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGos1PrP4y1o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zk16CLbZxjdW",
    "outputId": "a666d802-fee5-46fc-ff9e-89c217f41d2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000000 word vectors.\n",
      "comp 1008\n",
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 225)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 225, 300)     3000000     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 225, 300)     3000000     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 225, 300)     3000000     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d_18 (ZeroPadding1 (None, 229, 300)     0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d_19 (ZeroPadding1 (None, 233, 300)     0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d_20 (ZeroPadding1 (None, 229, 300)     0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d_21 (ZeroPadding1 (None, 233, 300)     0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d_22 (ZeroPadding1 (None, 229, 300)     0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d_23 (ZeroPadding1 (None, 233, 300)     0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 229, 16)      14416       zero_padding1d_18[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 233, 16)      24016       zero_padding1d_19[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 229, 16)      14416       zero_padding1d_20[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 233, 16)      24016       zero_padding1d_21[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 229, 16)      14416       zero_padding1d_22[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 233, 16)      24016       zero_padding1d_23[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 16)           0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 16)           0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 16)           0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 16)           0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 16)           0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_23 (Global (None, 16)           0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 96)           0           global_max_pooling1d_18[0][0]    \n",
      "                                                                 global_max_pooling1d_19[0][0]    \n",
      "                                                                 global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "                                                                 global_max_pooling1d_22[0][0]    \n",
      "                                                                 global_max_pooling1d_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          12416       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 4)            516         dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,128,228\n",
      "Trainable params: 9,128,228\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim,embedding_matrix=load_fast_test_embeddings(max_words, best_feat_list, tokenizer_index)\n",
    "text_seq_input = Input(shape=(maxlen,), dtype='int32')\n",
    "text_embedding1 = Embedding(max_words, embedding_dim, input_length=maxlen,weights=[embedding_matrix], trainable=True)(text_seq_input)\n",
    "text_embedding2 = Embedding(max_words, embedding_dim, input_length=maxlen)(text_seq_input)\n",
    "text_embedding3 = Embedding(max_words, embedding_dim, input_length=maxlen)(text_seq_input)\n",
    "\n",
    "filter_sizes = [3, 5]\n",
    "conv_pools = []\n",
    "for text_embedding in [text_embedding1, text_embedding2, text_embedding3]:\n",
    "  for filter_size in filter_sizes:\n",
    "    l_zero = ZeroPadding1D((filter_size - 1, filter_size - 1))(text_embedding)\n",
    "    l_conv = Conv1D(filters=16, kernel_size=filter_size, padding='same', activation='tanh')(l_zero)\n",
    "    l_pool = GlobalMaxPool1D()(l_conv)\n",
    "    conv_pools.append(l_pool)\n",
    "\n",
    "l_merge = Concatenate(axis=1)(conv_pools)\n",
    "l_dense = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(l_merge)\n",
    "l_out = Dense(4, activation='softmax')(l_dense)\n",
    "model = Model(inputs=[text_seq_input], outputs=l_out)\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fLycJAvM8-HN"
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "81lRvYIBl4Ux",
    "outputId": "1aab5f30-790d-43e0-96c9-3f80e62d7bd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "27/27 [==============================] - 2s 65ms/step - loss: 1.9434 - accuracy: 0.5802 - val_loss: 1.4845 - val_accuracy: 0.7302\n",
      "Epoch 2/20\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 1.1651 - accuracy: 0.8557 - val_loss: 1.0689 - val_accuracy: 0.8360\n",
      "Epoch 3/20\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 0.7400 - accuracy: 0.9443 - val_loss: 0.8346 - val_accuracy: 0.8571\n",
      "Epoch 4/20\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 0.4774 - accuracy: 0.9740 - val_loss: 0.6662 - val_accuracy: 0.8571\n",
      "Epoch 5/20\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 0.3118 - accuracy: 0.9832 - val_loss: 0.5584 - val_accuracy: 0.8677\n",
      "Epoch 6/20\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 0.2031 - accuracy: 0.9878 - val_loss: 0.6376 - val_accuracy: 0.7937\n",
      "Epoch 7/20\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 0.1441 - accuracy: 0.9847 - val_loss: 0.4751 - val_accuracy: 0.8519\n",
      "Epoch 8/20\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 0.1092 - accuracy: 0.9832 - val_loss: 0.5027 - val_accuracy: 0.8519\n",
      "Epoch 9/20\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 0.0881 - accuracy: 0.9847 - val_loss: 0.4295 - val_accuracy: 0.8677\n",
      "Epoch 10/20\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 0.0740 - accuracy: 0.9893 - val_loss: 0.4366 - val_accuracy: 0.8519\n",
      "Epoch 11/20\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 0.0654 - accuracy: 0.9863 - val_loss: 0.4929 - val_accuracy: 0.8466\n",
      "Epoch 12/20\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 0.0599 - accuracy: 0.9847 - val_loss: 0.4141 - val_accuracy: 0.8519\n",
      "Epoch 13/20\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 0.0540 - accuracy: 0.9885 - val_loss: 0.5546 - val_accuracy: 0.7989\n",
      "Epoch 14/20\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 0.0487 - accuracy: 0.9878 - val_loss: 0.4057 - val_accuracy: 0.8677\n",
      "Epoch 15/20\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 0.0432 - accuracy: 0.9893 - val_loss: 0.4875 - val_accuracy: 0.8571\n",
      "Epoch 16/20\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 0.0449 - accuracy: 0.9863 - val_loss: 0.4195 - val_accuracy: 0.8413\n",
      "Epoch 17/20\n",
      "27/27 [==============================] - 1s 36ms/step - loss: 0.0417 - accuracy: 0.9855 - val_loss: 0.4534 - val_accuracy: 0.8519\n",
      "Epoch 18/20\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 0.0375 - accuracy: 0.9901 - val_loss: 0.5450 - val_accuracy: 0.8360\n",
      "Epoch 19/20\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 0.0380 - accuracy: 0.9863 - val_loss: 0.4470 - val_accuracy: 0.8519\n",
      "Epoch 20/20\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 0.0366 - accuracy: 0.9870 - val_loss: 0.4608 - val_accuracy: 0.8519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4aa965ae10>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(X_train, y_train,epochs=20,batch_size=50,validation_data=(X_test, y_test),verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PngJl9n2n9f4"
   },
   "outputs": [],
   "source": [
    "model.save('/gdrive/My Drive/CDIP Dataset/cvl/text_model_cvl_cdip.hdf5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "text_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
